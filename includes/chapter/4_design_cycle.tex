\chapter{Artefakt-Design und Entwicklung}

Die Entwicklung des Artefakts folgte einem strukturierten Prozess, der sicherstellt, dass jede technische Entscheidung sowohl praxisnah als auch wissenschaftlich fundiert ist. In diesem Kapitel werden die Schritte zur Datenvorbereitung, Implementierung der Trainingspipeline und Hyperparameter-Optimierung detailliert diskutiert und begründet.

\section{Datensammlung und Analyse}

- Daten stammen aus Porsche Motorsport Cloud Plattform (ADX)
- Zugriff auf alle Sessions der Jahre 2023 bis 2025 der IMSA und WEC 
- Nur Rennsessions (keine Trainings- oder Qualifikationssessions), weil Reduzierung von Varianz durch unterschiedliche Fahrsituationen
- Zudem Filterung auf Runden mit trockenreifen ebenfalls zur Reduzierung von Varianz da Regenbedingunen andere Einflussfaktoren haben
- Diese Filterung erfolgt beim Abruf der Daten mittels ADX Kusto Query Language (KQL)
- Folgende Parameter wurden in dem Interview mit dem Performance Engineer als relevant identifiziert und werden deshalb als Features für das ML Modell verwendet:
- Parameter: kontinuierliche und kategoriale Features
- Parameter: 
Kontinuierlich: Umgebungstemperatur, Streckentemperatur, Windgeschwindigkeit, Reifentemperatur (für jeden Reifen einzeln), Reifendruck (für jeden Reifen einzeln), Fuel Load, Tyre Mileage Laps(für jeden Reifen einzeln)
kategorisch: Mechanical Balance (Anti-Roll-Bar-Front/Rear), Brake Balance, Traction Control, Tyre Compound, Track
- Zielvariable: aUndersteer\_AVG (Durchschnittlicher Wert pro Runde, $>0$ = Understeer, $<0$ = Oversteer)
- Alle Parameter werden über eine Runde gemittelt und ergeben so einen Datenpunkt pro Runde


- So ergibt sich ein Roh-Datensatz im CSV-Format mit 17735 Runden (Datenpunkte) 

- Explorative Datenanalyse (EDA) mittels Notebooks und Plots 
- Untersuchung der Verteilung der Rennrunden über die Events
- Untersuchung der Verteilung der Zielvariable
- Untersuchung der Verteilung der Features wie Reifentemperaturen, Reifendruck, Fuel Load
- Untersuchung von Korrelationen zwischen Features





\section{Datenvorbereitung und Feature-Engineering}

- Aus den Erkenntnisses der EDA werden verschiedene Vorverarbeitungsschritte abgeleitet
- Entfernung von redundanten Features (Reifentemperaturen, Reifendruck) anhand der Korrelationsmatrix und Erstellung von neuen abgeleiteten Features
- Festlegung von Thresholds für Ausreißer für Features
- Durch sehr unregelmäßigen Zielvariablen graph beschluss verschiedene Glättungen mittels gleitender Durchschnitte (Fenstergrößen 2, 3, 5, 8) und ohne zu testen
- Begründung der Glättungsentscheidungen: Die Wahl der Fenstergrößen basiert auf der Analyse der Zeitreihenstruktur der Zielvariablen. Kleinere Fenster erfassen kurzfristige Schwankungen, während größere Fenster langfristige Trends glätten.
- Umsetzung: Diese Vorverarbeitungsschritte werden in einem Skript \texttt{preprocess\_all\_events.py} implementiert.
- Der Track (Alphabestisch) zu TrackCode (numerische Codierung) und eine Mapping-Datei wird erstellt.
- One-Hot-Encoding nicht notwendig, da Baum-basierte Modelle wie XGBoost und LightGBM kategoriale Features nativ unterstützen\footnote{Vgl. Chen, Tianqi; Guestrin, Carlos 2016. XGBoost: A Scalable Tree Boosting System, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York ACM, S. 785-794}\footnote{Vgl. Ke, Guolin et al. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree, in Advances in Neural Information Processing Systems 30, La Jolla Curran Associates, S. 3146-3154}.
- Für die in Kapitel \ref{sec:validierung_modellvergleich} behandelte Validierung der Modelle werden separate Validierungsdatensätze erstellt:
- Dafür wird ein gesamtes Event nach der Leave-One-Out-Methode separiert
- Danach werden aus dem verbleibenden Datensatz 10\% zufällig ausgewählt und als Validierungsdatensatz verwendet.
- Beide Validierungsdatensätze hat das Model dem Training nicht gesehen.
- Resultierende Datensätze nach Vorverarbeitung und Aufteilung: 
- Daraus ergeben sich 10 Trainingsdatensätze: Mit und ohne kategoriale Features, jeweils mit 5 verschiedenen Glättungen (keine, 2, 3, 5, 8)
- Und 4 Validierungsdatensätze: Mit und ohne kategoriale Features, jeweils einmal für das Event und für den Zufallsdatensatz
- Resultierenden Datensatzgrößen:
- Trainingsdatensätze jeweils: ca. 9000 Runden (Datenpunkte)
- Validierungsdatensätze: Event: ca. 75 Runden (Datenpunkte) \& Zufallsdatensatz: ca. 1000 Runden (Datenpunkte)

\section{Modelltraining und Hyperparameter-Optimierung}

- Modelle: LightGBM (LGBMRegressor) und XGBoost (XGBRegressor)
- Begründung: Beide Algorithmen sind leistungsstarke, effiziente und weit verbreitete Implementierungen von Gradient Boosting Decision Trees, die sich besonders gut für strukturierte Daten eignen\footnote{Vgl. Chen, Tianqi; Guestrin, Carlos 2016. XGBoost: A Scalable Tree Boosting System, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York ACM, S. 785-794}\footnote{Vgl. Ke, Guolin et al. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree, in Advances in Neural Information Processing Systems 30, La Jolla Curran Associates, S. 3146-3154}.
- Beide Algorithmen unterstützen nativ die Verarbeitung von Kategorischen Features, was die Notwendigkeit für aufwändiges One-Hot-Encoding eliminiert und die Modellkomplexität reduziert
- Implementierung der Trainingspipeline in einem Skript \texttt{train\_model.py}
- 


Für XGBoost und LightGBM kommt ein strukturiertes GridSearchCV zum Einsatz\footnote{Vgl. Chen, Tianqi; Guestrin, Carlos 2016. XGBoost: A Scalable Tree Boosting System, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York ACM, S. 785-794}\footnote{Vgl. Ke, Guolin et al. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree, in Advances in Neural Information Processing Systems 30, La Jolla Curran Associates, S. 3146-3154}. Die Parameterbereiche sind in drei Komplexitätsstufen unterteilt:

\begin{itemize}
  \item \textbf{Shallow:} 50–100 Bäume, \texttt{max\_depth} 3–4
  \item \textbf{Medium:} 100–500 Bäume, \texttt{max\_depth} 5–9
  \item \textbf{Deep:} 300–700 Bäume, \texttt{max\_depth} 10–12
\end{itemize}

Diese Aufteilung basiert auf Benchmark-Ergebnissen, die einen Kompromiss zwischen Rechenaufwand und Modellgenauigkeit zeigen. In Gradient-Boosting-Verfahren sollten Bäume typischerweise eine geringe Tiefe zwischen 3 und 8 Ebenen haben, da vollständig gewachsene Bäume zu Overfitting führen würden\footnote{Vgl. Friedman, Jerome H. 2001. Greedy Function Approximation: A Gradient Boosting Machine, in The Annals of Statistics 29, 5, S. 1189-1232}. Schwache Lerner bei jedem Schritt helfen, Overfitting zu reduzieren\footnote{Vgl. ebd., S. 1203-1208}.

Jede Parameterkombination wird mittels dreifacher Cross-Validation über den R²-Score bewertet\footnote{Vgl. Pedregosa, Fabian et al. 2011. Scikit-learn: Machine Learning in Python, in Journal of Machine Learning Research 12, 1, S. 2825-2830}. K-Fold Cross-Validation ist die am häufigsten verwendete Methode zur Bestimmung der Wahrscheinlichkeit, dass ein Machine Learning-Ergebnis zufällig generiert wird\footnote{Vgl. Stone, Mervyn 1974. Cross-Validatory Choice and Assessment of Statistical Predictions, in Journal of the Royal Statistical Society 36, 2, S. 111-147}. Die erzielten Ergebnisse fließen in eine Tabelle mit den jeweils besten Parametern ein.

\section{Validierung und Modellvergleich}
\label{sec:validierung_modellvergleich}

Die abschließende Validierung erfolgt in einem dedizierten Jupyter-Notebook, das die finalen Modelle auf einem zuvor ungesehenen Validierungsdatensatz evaluiert und vergleichbar macht. Der Workflow gliedert sich in folgende Schritte:

\begin{itemize}
  \item \textbf{Verzeichnisstruktur und Dateipfade}\\
  Modelle liegen in Unterordnern von \texttt{../outputs/models/2/}. Zwei Validierungs-CSVs sind vorhanden: \texttt{val\_alldata.csv} (alle Features) und \texttt{val\_continuous.csv} (nur kontinuierliche Features).
  
  \item \textbf{Laden der Validierungsdaten}\\
  Für jede Modellvariante („alldata" vs. „continuous") wird das entsprechende CSV geladen. Vor der Vorhersage werden alle Spalten mit nur einem eindeutigen Wert entfernt, um irrelevante Features auszuschließen.
  
  \item \textbf{Modell-Loading und Vorhersage}\\
  In jedem Modellordner wird je ein XGBoost- (Dateiname \texttt{best\_xgb\_model.pkl}) und ein LightGBM-Modell (\texttt{best\_lgb\_model.pkl}) geladen. Mit dem jeweiligen Validierungs-Set werden Zielwerte \(y\) und Prädiktionen \(\hat{y}\) erzeugt.
  
  \item \textbf{Berechnung der Metriken}\\
  \begin{itemize}
    \item Mean Absolute Error (MAE)
    \item R²-Score (Bestimmtheitsmaß)
  \end{itemize}
  Die Ergebnisse werden in einer Tabelle \texttt{results\_df} gespeichert mit den Spalten \texttt{Modellordner}, \texttt{Modelltyp}, \texttt{Validation-CSV}, \texttt{MAE}, \texttt{R2}. Der R²-Score misst den Anteil der Varianz in der abhängigen Variable, der durch die unabhängigen Variablen vorhersagbar ist, wobei der bestmögliche Wert 1.0 beträgt\footnote{Vgl. James, Gareth et al. 2021. An Introduction to Statistical Learning. with Applications in R. 2. Aufl. New York Springer, S. 68-74}.
  
  \item \textbf{Top-5-Auswahl nach R² und MAE}\\
  \texttt{top\_r2}: die fünf besten Modelle nach absteigendem R²\\
  \texttt{top\_mae}: die fünf besten Modelle nach aufsteigendem MAE
  
  \item \textbf{Konsolidierte Rangfolge}\\
  Jedes Modell erhält einen Rang in beiden Top-5-Listen (Platz 0–4; außerhalb = 5). Aus dem Mittelwert dieser beiden Ränge wird eine finale Liste der fünf besten Modelle (\texttt{rankings\_df}) erstellt.
\end{itemize}

\noindent
Dadurch wird transparent, welche Modelltyp-/Feature-Kombination auf neuen, ungesehenen Telemetriedaten am besten generalisiert. Die Generalisierungsfähigkeit ist die Fähigkeit eines trainierten Modells, genaue Vorhersagen für neue, ungesehene Daten zu treffen\footnote{Vgl. Vapnik, Vladimir N. 2013. The Nature of Statistical Learning Theory. 2. Aufl. New York Springer, S. 15-28}.
