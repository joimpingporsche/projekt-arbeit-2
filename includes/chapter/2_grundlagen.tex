\chapter{Theoretische Grundlagen}

Zur Entwicklung eines \ac{ML}-Modells für Motorsport-Telemetrie-Vorhersage 
müssen mehrere ineinandergreifende Konzepte verstanden werden. Dieses Kapitel 
vermittelt zunächst methodische Grundlagen zu Design Science Research und Experteninterviews, dann die Theorie von Regressionsproblemen und speziell 
Gradient Boosting Decision Trees, und abschließend praktische Aspekte von 
Optimierung und Validierung. Diese Grundlagen bilden die wissenschaftliche Basis 
für die in Kapitel 4 beschriebene Artefakt-Entwicklung.


\section{Design Science Research Methodik}

\ac{DSR} ist ein etablierter Forschungsansatz, der sich grundlegend von deskriptiven Forschungsmethoden unterscheidet. Während traditionelle empirische Forschung primär darauf ausgerichtet ist, bestehende Phänomene zu verstehen und zu erklären, zielt \ac{DSR} darauf ab, praktische Probleme durch systematische Entwicklung und rigorose Evaluierung von Artefakten zu lösen. Damit schafft \ac{DSR} eine Brücke zwischen wissenschaftlicher Fundierung und praktischer Problemlösung.\footnote{Vgl. \cite{Hevner2004}, S. 82}

Das Grundwerk von Hevner et al. (2004) prägt bis heute das Verständnis von 
\ac{DSR} und etabliert ein Framework, das auf drei ineinandergreifenden Zyklen 
basiert:\footnote{Vgl. \cite{Hevner2004}, S. 82 ff.}

Der \textit{Relevance Cycle} beginnt mit Problemidentifikation aus der 
Anwendungsdomäne; der \textit{Rigor Cycle} verankert die Entwicklung in 
wissenschaftlichem Wissen und etablierten Theorien; der \textit{Design Cycle} 
orchestriert iterative Phasen von Artefakt-Konzeption, Entwicklung und 
Evaluierung. Diese drei Zyklen ermöglichen eine systematische und 
nachvollziehbare Forschungsvorgehensweise, bei der wissenschaftliche Strenge 
nicht auf Kosten von Praxisrelevanz geht.

Operationalisiert wird dieser Zyklus-Framework durch sechs sequenzielle 
\textit{Phasen}, die das praktische Vorgehen konkretisieren:\footnote{Vgl. 
\cite{Hevner2004}, S. 90.} 

\begin{enumerate}
  \item \textbf{Problem Identification and Motivation:} Analyse der 
  Problemdomäne und Begründung ihrer wissenschaftlichen und praktischen Relevanz.
  
  \item \textbf{Definition of Objectives:} Spezifizierung der Anforderungen, 
  die das entwickelte Artefakt erfüllen muss.
  
  \item \textbf{Design and Development:} Konzeption und prototypische 
  Implementierung des Artefakts.
  
  \item \textbf{Demonstration:} Nachweis, dass das Artefakt das Problem 
  tatsächlich lösen kann, typischerweise durch Fallstudien oder kontrollierte 
  Szenarien.
  
  \item \textbf{Evaluation:} Systematische Bewertung des Artefakts gegen die 
  vordefinierten Anforderungen und Ziele.
  
  \item \textbf{Communication:} Mitteilung von Erkenntnissen und Design 
  Knowledge an die wissenschaftliche Gemeinschaft.
\end{enumerate}

Diese sechs Phasen laufen gleichzeitig in den drei Zyklen ab: Der Relevance 
Cycle informiert die Problemidentifikation (Phase 1), der Design Cycle orchestriert 
die Phasen 2–5 iterativ, und der Rigor Cycle durchzieht alle Phasen mit 
wissenschaftlicher Strenge.

Artefakte in \ac{DSR} können verschiedene Formen annehmen. Constructs sind konzeptionelle Vokabularien und Abstraktionen, die Probleme präzise definieren. Models stellen Zusammenhänge und Strukturen in vereinfachter Form dar. Methods sind Algorithmen und systematische Verfahrensweisen zur Problemlösung. Instantiations schließlich sind konkrete Implementierungen oder Prototypen.\footnote{Vgl. \cite{Hevner2004}, S. 82 ff.} In \ac{ML}-Projekten ist die Instantiation typischerweise ein trainiertes Modell mit vollständiger Pipeline (Datenvorverarbeitung, Feature Engineering, trainierte Parameter). Die vorliegende Arbeit entwickelt eine Instantiation: ein evaluiertes \ac{ML}-Modell für Fahrzeugbalance-Vorhersage.

Die Evaluierung in \ac{DSR} erfüllt mehrere funktionale Rollen. Sie stellt fest, ob und inwieweit das Artefakt die definierten Anforderungen erfüllt. Sie identifiziert Verbesserungspotenziale für weitere Iterationen. Vor allem trägt sie zur wissenschaftlichen Wissensbasis bei, indem Design Principles und generalisierbare Lessons Learned dokumentiert werden.\footnote{Vgl. \cite{Venable2016}, S. 79 ff.; vgl. dazu auch \cite{Gregor2007}, S. 322 ff., sowie \cite{Hevner2004}, S. 84 f.}
 Bewährte Evaluationsmethoden in \ac{DSR} sind observational (Feldbeobachtung), 
analytical (logische Deduktion und Proof-of-Concept), experimental 
(kontrollierte Experimente mit Baseline-Vergleich), testing (systematische 
Funktionsprüfung) und descriptive (qualitative Bewertung durch Experten).\footnote{Vgl. 
\cite{Hevner2004}, S. 85 f.} Für \ac{ML}-Artefakte dominieren 
analytische und experimentelle Evaluationen mittels etablierter Performance-Metriken.\footnote{Vgl. \cite{Friedman2009}, S. 219 ff.}


\section{Experteninterviews zur Anforderungsermittlung}

Die Anforderungsanalyse in Kapitel 3 basiert auf zwei informellen Gesprächen mit einer erfahrenen Renningenieur*in aus dem Porsche Motorsport-Team. Das erste Gespräch fand am 29.08.2025 statt, das zweite am 12.09.2025. Ziel war es, durch offene Diskussion die tägliche Arbeitsweise der Renningenieur*innen nachzuvollziehen, zentrale technische Herausforderungen zu identifizieren und realistische Anforderungen an ein Vorhersagemodell zu formulieren.
Die interviewte Expert*in agiert als Teamleiter*in der Performance-Abteilung im Porsche \ac{LMDh}-Programm und ist verantwortlich für die Fahrzeugentwicklung, Simulationsmodellierung und insbesondere die datenbasierte Optimierung des Rennwagen-Setups während des operativen Betriebs, wodurch sie über tiefgehendes, direkt anwendbares Wissen im Telemetrie-Management verfügt.\footnote{Vgl. Experteninterview 1, 29.08.2025, Z. 3-27}
Beide Gespräche folgten einem offenen, exploratorischen Format ohne strukturierten Fragenkatalog, um eine natürliche Konversation zu fördern und implizites Wissen der Expert*in zugänglich zu machen.\footnote{Vgl. \cite{Döringer2021}, S. 2 ff.} Die gewonnenen Erkenntnisse bestätigten, dass die aktuelle Telemetrie-Analyse stark manuell erfolgt. Dies validierte die Problemrelevanz und leitete die Anforderungsableitung in Kapitel 3.\footnote{Vgl. Experteninterview 1 und 2, dokumentiert in Anhang A.1} 



\section{Maschinelles Lernen für Regressionsprobleme}
Maschinelles Lernen (\ac{ML}) beschreibt die Fähigkeit von Algorithmen, aus Beispieldaten Muster zu erkennen und daraus Vorhersagemodelle zu erstellen, ohne dass die Regeln explizit programmiert werden müssen.\footnote{Vgl. \cite{Mitchell1997}, S. 1 f.} In diesem Projekt geht es darum, aus Telemetriedaten eines Fahrzeugs eine kontinuierliche Zielgröße vorherzusagen: den Fahrzeugbalance-Wert. Dies ist ein typisches Regressionsproblem, bei dem die Zielvariable nicht diskret (wie bei einer Klassifikation), sondern stetig ist.\footnote{Vgl. \cite{Hastie2009}, S. 1 ff.}
Regressionsmodelle gehören zum überwachten Lernen: Für jede Trainingsinstanz ist der korrekte Zielwert bekannt. Das Modell lernt, diesen Wert aus den Eingabedaten zu rekonstruieren und soll später auch auf neue, unbekannte Daten verallgemeinern können.
Damit ein Modell erfolgreich lernen kann, ist die Datenvorbereitung entscheidend. Sie beginnt mit einer explorativen Datenanalyse (EDA), bei der die Rohdaten zunächst gesichtet werden, um Verteilungen, Korrelationen und potenzielle Ausreißer zu erkennen.\footnote{Vgl. \cite{Tukey1977}, S. 125 ff.} Darauf folgt die Datenbereinigung, bei der fehlende Werte durch Imputation oder Ausschluss behandelt, fehlerhafte Messungen entfernt und sachlogische Schwellenwerte gesetzt werden, etwa zur Filterung unplausibler Sensorwerte.\footnote{Vgl. \cite{Kuhn2019}, S. 20 ff.} Im nächsten Schritt erfolgt das Feature Engineering, bei dem die Rohdaten in aussagekräftige Eingabemerkmale transformiert werden. Relevante Features werden gezielt ausgewählt, um Overfitting zu vermeiden und die Trainingszeit zu reduzieren.\footnote{Vgl. \cite{Guyon2003}, S. 1157 ff.} Hochkorrelierte Sensoren können zu aggregierten Merkmalen zusammengefasst werden, etwa durch Mittelwertbildung mehrerer Temperaturmessungen. Kategoriale Variablen wie Streckenkennungen werden durch Encoding in numerische Form überführt.\footnote{Vgl. \cite{Kuhn2019}, S. 139 ff.} Für Telemetriedaten ist zudem eine Glättung von Zeitreihen sinnvoll, beispielsweise durch gleitende Mittelwerte, um hochfrequentes Rauschen zu reduzieren und echte physikalische Trends von Messfehlern zu unterscheiden.\footnote{Vgl. \cite{Box2015}, S. 25 ff.; vgl. dazu auch \cite{Cleveland1979}, S. 829 ff.}

Nach der Datenvorbereitung stellt sich die Frage, welche Modellklasse für das Problem geeignet ist. 
Im nächsten Abschnitt wird die Methode der Gradient Boosting Decision Trees (GBDTs) vorgestellt, die sich besonders gut für strukturierte Telemetriedaten eignet und typische Anforderungen des vorliegenden Problems adressiert.

\section{Gradient Boosting Decision Trees}

Gradient Boosting Decision Trees (GBDT) sind eine Ensemble-Methode, die sequenzielle schwache Lerner, das heißt einfache Decision Trees, zu einem starken Vorhersagemodell kombiniert.\footnote{Vgl. \cite{Friedman2001}, S. 1189 ff.} Das Kernprinzip ist Residual Learning: Der erste Baum wird auf die Rohdaten trainiert. Der zweite Baum wird trainiert, um die Fehler des ersten Baums vorherzusagen, die sogenannten Residuen. Der dritte Baum korrigiert dann die kombinierten Fehler von Baum 1 und 2, und so weiter. Die finale Vorhersage ergibt sich aus einer gewichteten Summe aller Baum-Ausgaben.
Formalisiert wird dies durch Gradient Descent: Eine Loss-Funktion, typischerweise Mean Squared Error für Regression, quantifiziert, wie schlecht die Vorhersagen sind. Jeder neue Baum wird so konstruiert, dass er in Richtung des negativen Gradienten dieser Loss-Funktion läuft, also gezielt Fehler reduziert.\footnote{Vgl. \cite{Friedman2001}, S. 1200 ff.} Dieser Gradient-Ansatz führt zu schnellerer Konvergenz als alternatives Ensemble-Averaging.

Gradient Boosting Decision Trees (GBDTs) haben sich als besonders geeignet für 
Regressionsprobleme erwiesen.\footnote{Vgl. \cite{Friedman2001}, S. 1189 ff.; 
vgl. dazu auch \cite{Chen2016}, S. 785 ff.} Sie erfassen nichtlineare 
Zusammenhänge zwischen Eingabemerkmalen und Zielvariable, was über klassische 
lineare Modelle hinausgeht.\footnote{Vgl. \cite{Hastie2009}, S. 58 ff.} Gleichzeitig funktionieren sie 
effizient auf Datensätzen moderater Größe, im Gegensatz zu neuronalen Netzen, die 
oft große Datenmengen benötigen.\footnote{Vgl. \cite{Goodfellow2016}, S. 164 ff.} GBDTs sind zudem vergleichsweise schnell zu trainieren 
und bieten Interpretierbarkeit durch Feature-Importance-Schätzungen, die aufzeigen, 
welche Telemetrie-Signale besonders stark zur Vorhersage der Fahrzeugbalance 
beitragen.\footnote{Vgl. \cite{Chen2016}, S. 788 ff.}

Zwei verbreitete Implementierungen von Gradient Boosting sind XGBoost und 
LightGBM.\footnote{Vgl. \cite{Chen2016}, S. 785 ff.; vgl. dazu auch \cite{Ke2017}, S. 3146 ff.} Beide verfolgen dasselbe Grundprinzip, sequenzielles 
Trainieren von Bäumen zur Fehlerkorrektur, unterscheiden sich aber in technischen 
Details und Optimierungen.

XGBoost ist eine etablierte und vielseitige Bibliothek, die sich durch robuste Regularisierung und gute Performance auszeichnet.\footnote{Vgl. \cite{Chen2016}, S. 785 ff.}
LightGBM wurde später entwickelt und ist besonders auf Geschwindigkeit und Effizienz bei großen Datensätzen ausgelegt.\footnote{Vgl. \cite{Ke2017}, S. 3146 ff.}

Beide Tools sind für strukturierte Daten geeignet und haben sich in der Praxis bewährt. Die konkrete Wahl hängt vom Anwendungsfall ab und wird im folgenden Kapitel diskutiert.
Empirisch zeigen beide Implementierungen überlegene Performance auf strukturierten tabularen Daten im Vergleich zu tiefen neuronalen Netzen.\footnote{Vgl. \cite{Chen2016}, S. 788 ff.} Die Wahl zwischen ihnen ist oft pragmatisch: XGBoost für Balance, LightGBM wenn Geschwindigkeit kritisch ist.



\section{Hyperparameter-Optimierung und Modellvalidierung}

Um ein GBDT-Modell zu trainieren, müssen vor dem Training viele Hyperparameter gesetzt werden. Hyperparameter unterscheiden sich fundamental von Modell-Parametern, den internen Gewichten des Modells, die während Training gelernt werden. Hyperparameter sind konfigurierbare Stellschrauben, die Entwickler*innen vor dem Training definieren und die das Lernverhalten des Algorithmus steuern.\footnote{Vgl. \cite{Bergstra2012}, S. 281 ff.}

Für GBDT sind mehrere Hyperparameter besonders wichtig: Die Anzahl der Estimators bestimmt, wie viele Bäume insgesamt trainiert werden, während die Learning Rate als Schrittweite beim Gradientenverfahren fungiert, wobei kleine Werte zu langsameren, stabileren Verbesserungen, große Werte zu schnellerem, aber riskanterem Lernen führen. Die Max Depth definiert die maximale Tiefe jedes Baums und erhöht mit steigenden Werten die Modellkomplexität. Das Min Child Weight (bzw. die Mindestanzahl an Samples pro Blatt) bewirkt bei höheren Werten konservativere Splits und damit weniger Overfitting, während Subsample den Anteil der Trainingsinstanzen pro Baum festlegt. Schließlich kontrollieren Regularisierungsparameter wie L2-Penalisierung die Modellkomplexität.\footnote{Vgl. \cite{Chen2016}, S. 787 f.}
Da manuelle Anpassung dieser Parameter ineffizient ist, nutzt man systematische Hyperparameter-Optimierung. Grid Search definiert ein vorgegebenes Gitter von Parameterwerten und probiert alle Kombinationen systematisch durch, garantiert zwar eine gründliche Suche, führt aber zu exponentiellem Rechenaufwand mit der Anzahl der Parameter.\footnote{Vgl. \cite{Bergstra2012}, S. 281 ff.; vgl. dazu auch \cite{Chen2016}, S. 787 f.}
Nach der Hyperparameter-Optimierung ergibt sich eine weitere zentrale Herausforderung: Generalisiert das Modell auf unbekannte Daten? Diese wird durch Modellvalidierung beantwortet. Der Standard k-fold Cross-Validation partitioniert den Datensatz in k gleiche Teile und trainiert das Modell k-mal, jeweils mit k minus eins Teilen als Training und einem Teil als Validierung. Die gemittelten k Validierungs-Scores geben eine robuste Schätzung der Generalisierungsperformance mit geringerer Varianz als ein einzelner Train-Test-Split.\footnote{Vgl. \cite{Kohavi1995}, S. 1137 ff.}

Im vorliegenden Projekt ist jedoch die kritischste Generalisierungsfrage der Transfer auf komplett neue Rennevents, da eine neue Veranstaltung völlig andere Umgebung, andere Fahrer und anderes Setup aufweist. Standard-Cross-Validation löst diese Herausforderung nicht, weshalb hier Leave-One-Out verwendet wird. Dafür wird das Modell auf einem Trainingsdatensatz trainiert, der alle Veranstaltungen außer einer enthält, und dann auf der zurückgehaltenen Veranstaltung validiert.\footnote{Vgl. \cite{James2021}, S. 176 ff.} Dies stellt einen extremen, aber realistischen Stress-Test für echte Domänen-Generalisierung dar und offenbart, ob das Modell echte physikalische Strukturen gelernt hat oder nur Muster des Trainingsdatensatzes memoriert hat.

Diese drei Komponenten Hyperparameter-Optimierung, Cross-Validation Strategie und Evaluationsmetrik sind untrennbar: Zusammen stellen sie sicher, dass ein entwickeltes Modell tatsächlich verallgemeinerbar ist, nicht nur auf Trainingsdaten overfittet.


\section{Evaluationsmetriken für Regression}

Regressionsergebnisse werden durch mehrere etablierte Fehlermetriken quantifiziert. Der \ac{MSE} berechnet den Durchschnitt der quadrierten Abweichungen zwischen Vorhersagen und Ist-Werten und bestraft größere Fehler überproportional. Der \ac{RMSE} ist die Quadratwurzel des MSE und hat dieselbe Einheit wie die Zielvariable, was die Interpretation erleichtert.\footnote{Vgl. \cite{Hodson2022}, S. 5481 f.}

Der \ac{MAE} quantifiziert den Durchschnitt der absoluten Abweichungen und ist robuster gegenüber Ausreißern, da er Fehler linear (nicht quadratisch) gewichtet.\footnote{Vgl. \cite{ChaiDraxler2014}, S. 1247 ff.} Die Wahl zwischen RMSE und MAE sollte sich nach der erwarteten Fehlerverteilung richten: RMSE ist optimal bei normalverteilten Fehlern, MAE bei Laplace-verteilten Fehlern.

Das Bestimmtheitsmaß R² (Coefficient of Determination) gibt an, welcher Anteil der Varianz der Zielvariable durch das Modell erklärt wird. R² = 1 signalisiert perfekte Vorhersagen, R² = 0 bedeutet, dass das Modell nicht besser als die Mittelwert-Baseline ist. Negative R²-Werte sind möglich und deuten auf schlechtere Performance als die Baseline hin.\footnote{Vgl. \cite{Hastie2009}, S. 10 ff.} In der Ingenieurpraxis und insbesondere im Motorsport-Datenkontext werden für 
Validierungsmodelle üblicherweise Schwellwerte von R² $\geq$ 0,7 angestrebt, 
da dies eine Fehlerreduktion von etwa 50\% gegenüber einem Baseline-Modell 
bedeutet und damit praktische Einsatzfähigkeit gewährleistet.\footnote{Vgl. \cite{ODonnell2024}, S. 1 ff.}


Diese drei Metriken (RMSE, MAE, R²) bilden den internationalen Standard in der Regressionsanalyse und ermöglichen Vergleichbarkeit mit etablierten Benchmarks in der Fachliteratur.