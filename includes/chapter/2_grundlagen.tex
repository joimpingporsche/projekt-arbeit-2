\chapter{Theoretische Grundlagen}

\section{Design Science Research Methodologie}

\ac{DSR} ist ein etablierter Forschungsansatz der Wirtschaftsinformatik, der sich grundlegend von deskriptiven Forschungsmethoden unterscheidet. Während traditionelle empirische Forschung primär darauf ausgerichtet ist, bestehende Phänomene zu verstehen und zu erklären, zielt \ac{DSR} darauf ab, praktische Probleme durch systematische Entwicklung und rigorose Evaluierung von Artefakten zu lösen. Damit schafft \ac{DSR} eine Brücke zwischen wissenschaftlicher Fundierung und praktischer Problemlösung.\footnote{Vgl. Hevner et al. 2004, S. 77-83}

Das Grundwerk von Hevner et al. (2004) prägt bis heute das Verständnis von \ac{DSR} und etabliert ein Framework, das auf sieben präskriptiven Richtlinien basiert. Das Framework schreibt vor, dass \ac{DSR}-Projekte in drei ineinandergreifenden Zyklen durchgeführt werden sollten: Der Relevance Cycle beginnt mit Problemidentifikation aus der Anwendungsdomäne; der Rigor Cycle verankert die Entwicklung in wissenschaftlichem Wissen und etablierten Theorien; der Design Cycle orchestriert iterative Phasen von Artefakt-Konzeption, Entwicklung und Evaluierung. Diese drei Zyklen ermöglichen eine systematische und nachvollziehbare Forschungsvorgehensweise, bei der wissenschaftliche Strenge nicht auf Kosten von Praxisrelevanz geht.\footnote{Vgl. Hevner et al. 2004, S. 77-92}

Der \ac{DSR}-Prozess gliedert sich typischerweise in sechs sequenzielle Phasen. Problem Identification and Motivation beginnt mit der Analyse der Problemdomäne und Begründung ihrer wissenschaftlichen und praktischen Relevanz. Definition of Objectives spezifiziert die Anforderungen, die das entwickelte Artefakt erfüllen muss. In der Design and Development Phase wird das Artefakt konzipiert und prototypisch implementiert. Die Demonstration Phase dokumentiert, dass das Artefakt das Problem tatsächlich lösen kann, typischerweise durch Fallstudien oder kontrollierte Szenarien. Die Evaluation Phase bewertet das Artefakt systematisch gegen die vordefinierten Anforderungen und Ziele. Abschließend erfolgt die Communication Phase, in der Erkenntnisse und Design Knowledge der wissenschaftlichen Gemeinschaft mitgeteilt werden.\footnote{Vgl. Prat et al. 2014, S. 1-16}

Artefakte in \ac{DSR} können verschiedene Formen annehmen. Constructs sind konzeptionelle Vokabularien und Abstraktionen, die Probleme präzise definieren. Models stellen Zusammenhänge und Strukturen in vereinfachter Form dar. Methods sind Algorithmen und systematische Verfahrensweisen zur Problemlösung. Instantiations schließlich sind konkrete Implementierungen oder Prototypen.\footnote{Vgl. Hevner et al. 2004, S. 80} In ML-Projekten ist die Instantiation typischerweise ein trainiertes Modell mit vollständiger Pipeline (Datenvorverarbeitung, Feature Engineering, trainierte Parameter). Die vorliegende Arbeit entwickelt eine Instantiation: ein evaluiertes Machine-Learning-Modell für Fahrzeugbalance-Vorhersage.

Die Evaluierung in \ac{DSR} erfüllt mehrere funktionale Rollen. Sie stellt fest, ob und inwieweit das Artefakt die definierten Anforderungen erfüllt. Sie identifiziert Verbesserungspotenziale für weitere Iterationen. Vor allem trägt sie zur wissenschaftlichen Wissensbasis bei, indem Design Principles und generalisierbare Lessons Learned dokumentiert werden.\footnote{Vgl. Venable et al. 2016, S. 77-89} Bewährte Evaluationsmethoden in \ac{DSR} sind observational (Feldbeobachtung), 
analytical (logische Deduktion und Proof-of-Concept), experimental 
(kontrollierte Experimente mit Baseline-Vergleich), testing (systematische 
Funktionsprüfung) und descriptive (qualitative Bewertung durch Experten).\footnote{Vgl. 
Hevner et al. 2004, S. 85-86} Für Machine-Learning-Artefakte dominieren 
analytische und experimentelle Evaluationen mittels etablierter Performance-Metriken.\footnote{Vgl. 
Prat et al. 2014, S. 9-12}


\section{Experteninterviews zur Anforderungsermittlung}

Die Anforderungsanalyse in Kapitel 3 basiert auf zwei informellen Gesprächen mit einem erfahrenen Performance Engineer aus dem Porsche Motorsport-Team. Das erste Gespräch fand am 29.08.2025 statt, das zweite am 12.09.2025. Ziel war es, durch offene Diskussion die tägliche Arbeitsweise im Telemetrie-Management nachzuvollziehen, zentrale technische Herausforderungen zu identifizieren und realistische Anforderungen an ein automatisiertes Vorhersagemodell zu formulieren.
Der interviewte Experte agiert als Teamleiter der Performance-Abteilung im Porsche LMDh-Programm und ist verantwortlich für die Fahrzeugentwicklung, Simulationsmodellierung und insbesondere die datenbasierte Optimierung des Rennwagen-Setups während des operativen Betriebs, wodurch er über tiefgehendes, direkt anwendbares Wissen im Telemetrie-Management verfügt.

Beide Gespräche folgten einem offenen, exploratorischen Format ohne strukturierten Fragenkatalog, um eine natürliche Konversation zu fördern und implizites Wissen des Experten zugänglich zu machen. Die gewonnenen Erkenntnisse bestätigten, dass die aktuelle Telemetrie-Analyse stark manuell erfolgt und mehrere Stunden pro Rennwochenende erfordert. Dies validierte die Problemrelevanz und leitete die Anforderungsableitung in Kapitel 3.\footnote{Vgl. Experteninterview 1 und 2, dokumentiert in Anhang A.1} 



\section{Maschinelle Lernverfahren für Regressionsprobleme}

Machine Learning wird allgemein als automatische Induktion von vorhersagenden Modellen aus Daten definiert, ohne dass Algorithmen explizit programmiert werden müssen.\footnote{Vgl. Mitchell 1997, S. 1-2} Im praktischen Kontext bedeutet dies: Ein Lernalgorithmus erhält Trainingsdaten, erkennt Muster in diesen Daten und extrahiert eine verallgemeinerbare mathematische Struktur, die auf neue, unsichtbare Daten angewendet werden kann.

Das vorliegende Projekt verfolgt ein Regressionsziel: Vorhersage einer kontinuierlichen Zielvariable (Fahrzeugbalance-Wert) aus einer Menge strukturierter Input-Features (Telemetrie-Metriken). Dies unterscheidet sich von Klassifikation, bei der diskrete Kategorien vorhergesagt werden. Regression ist ein Supervised-Learning-Problem: Jede Trainingsinstanz hat ein bekanntes, korrektes Label (den gemessenen Fahrzeugbalance-Wert), gegen das das Modell seine Vorhersagen abgleichen kann.\footnote{Vgl. Hastie et al. 2009, S. 1-25}

Das zentrale Problem beim Modelllernen wird durch den Bias-Variance Trade-off beschrieben. Ein Modell mit niedriger Komplexität wie lineare Regression hat hohen Bias: Es macht systematisch vereinfachte Vorhersagen, die die tatsächliche nichtlineare Beziehung zwischen Features und Zielvariable nicht erfassen und somit zu Underfitting führen. Umgekehrt weist ein hochkomplexes Modell wie ein überparametrisiertes Polynom hohe Varianz auf: Es memoriert Trainingsrauschen und besondere Trainingsfälle und generalisiert dadurch schlecht auf neue Daten, was als Overfitting bezeichnet wird.\footnote{Vgl. Hastie et al. 2009, S. 23-31} Das Ziel ist ein gutes Gleichgewicht: Ein Modell mit ausreichender Komplexität, um die echte Struktur der Daten zu erfassen, aber nicht so komplex, dass es Zufallsrauschen memoriert.

Die Vorbereitung der Daten ist entscheidend für späteren Erfolg. Exploratory Data Analysis (EDA) untersucht zunächst die Rohverteilungen der Features, identifiziert Korrelationen zwischen Variablen und erkennt potenzielle Ausreißer oder Anomalien.\footnote{Vgl. Carvajal-Gómez et al. 2024, S. 1-15} Data Cleaning adressiert praktische Datenqualitätsprobleme: fehlende Werte werden behandelt durch Imputation oder Ausschluss, anomale Messwerte werden gefiltert, und sachlogische Schwellwerte werden gesetzt, etwa der Ausschluss von Messungen mit unplausiblen Sensorwerten.\footnote{Vgl. Puschmann, Burghardt 2022, S. 1-12}

Feature Engineering ist der kreative Schritt, in dem Rohdaten in aussagekräftige Prädiktoren transformiert werden. Feature Selection wählt von vielen möglichen Kandidaten diejenigen aus, die am meisten zur Vorhersage beitragen und reduziert dadurch Overfitting sowie Trainingszeit.\footnote{Vgl. Guyon, Elisseeff 2003, S. 1157-1182} Aggregation kombiniert hochkorrelierte Sensoren zu zusammengefassten Features, beispielsweise der Durchschnitt mehrerer Temperatur-Sensoren, um Rauschredundanz zu minimieren.\footnote{Vgl. Guyon, Elisseeff 2003, S. 1157-1182} Encoding wandelt kategoriale Variablen wie Track-Identifikatoren in numerische Repräsentationen um.\footnote{Vgl. Abraham 2025, S. 1-18} Für Telemetriedaten ist Zeitreihen-Glättung wie Moving Averages essentiell: Sie reduziert hochfrequentes Sensorrauschen, ohne Trends zu zerstören, und ermöglicht es, echte physikalische Änderungen von Messfehlern zu unterscheiden.\footnote{Vgl. Lee et al. 2022, S. 1-15}

Nach der Datenvorbereitung entsteht eine zentrale Designfrage: Welcher Regressionsalgorithmus ist am besten geeignet? Lineare Modelle wie Linear Regression, Ridge und Lasso bieten exzellente Interpretierbarkeit – es ist klar, welche Features wie stark die Vorhersage beeinflussen – können aber intrinsisch nur lineare Beziehungen erfassen.\footnote{Vgl. Hastie et al. 2009, S. 58-85} Support Vector Regression nutzt mathematische Tricks, die sogenannten Kernel-Tricks, um implizit nichtlineare Feature-Transformationen durchzuführen, bleibt aber schwer zu interpretieren.\footnote{Vgl. Hastie et al. 2009, S. 290-310} Decision Trees sind intuitiv: Sie partitionieren den Feature-Raum sequenziell anhand scharfer Grenzen, etwa wenn Feature X größer als 5 ist, dann gehe linken Ast. Sie sind robust und können komplexe nichtlineare Muster erfassen, leiden aber unter Overfitting, da ein einzelner Baum Trainingsrauschen memoriert.\footnote{Vgl. Breiman 2001, S. 5-32} Random Forests verbessern einzelne Bäume durch Ensemble-Averaging: Viele verschiedene Bäume werden auf zufällig gestörten Datensätzen trainiert, und ihre Vorhersagen werden gemittelt, was Varianz reduziert und Generalisierung verbessert.\footnote{Vgl. Breiman 2001, S. 5-32} Boosting-Methoden folgen einem anderen Ensemble-Prinzip: Sie trainieren Bäume sequenziell, wobei jeder neue Baum systematisch die Fehler vorheriger Bäume korrigiert.\footnote{Vgl. Friedman 2001, S. 1189-1232} Neuronale Netze können beliebig komplexe Funktionen approximieren und funktionieren gut bei großen Datenmengen, erfordern aber typischerweise mehr Trainingsdaten als Traditional-ML-Methoden und sind weniger interpretierbar.\footnote{Vgl. Goodfellow et al. 2016, S. 164-223}

Die Frage stellt sich: Welche Methode eignet sich für das vorliegende Motorsport-Problem mit strukturierten, tabulischen Daten von moderater Größe mit 17.735 Runden von 40 Rennveranstaltungen, in denen konkurrierende komplexe Effekte zwischen Features existieren und Interpretierbarkeit gewünscht ist, um Engineers zu unterstützen? Unter diesen Bedingungen haben sich Gradient Boosting Decision Trees als Methode der Wahl etabliert, weil sie nichtlineare Muster erfassen, auf moderaten Datenmengen gut funktionieren und ein gutes Gleichgewicht zwischen Vorhersagegenauigkeit und Interpretierbarkeit bieten.\footnote{Vgl. Chen, Guestrin 2016, S. 785-794}



\section{Gradient Boosting Decision Trees: Theorie und Implementierungen}

Gradient Boosting Decision Trees (GBDT) sind eine Ensemble-Methode, die sequenzielle schwache Lerner, das heißt einfache Decision Trees, zu einem starken Vorhersagemodell kombiniert.\footnote{Vgl. Friedman 2001, S. 1189-1232} Das Kernprinzip ist Residual Learning: Der erste Baum wird auf die Rohdaten trainiert. Der zweite Baum wird trainiert, um die Fehler des ersten Baums vorherzusagen, die sogenannten Residuen. Der dritte Baum korrigiert dann die kombinierten Fehler von Baum 1 und 2, und so weiter. Die finale Vorhersage ergibt sich aus einer gewichteten Summe aller Baum-Ausgaben.

Formalisiert wird dies durch Gradient Descent: Eine Loss-Funktion, typischerweise Mean Squared Error für Regression, quantifiziert, wie schlecht die Vorhersagen sind. Jeder neue Baum wird so konstruiert, dass er in Richtung des negativen Gradienten dieser Loss-Funktion läuft, also gezielt Fehler reduziert.\footnote{Vgl. Friedman 2001, S. 1200-1210} Dieser Gradient-Ansatz führt zu schnellerer Konvergenz als alternatives Ensemble-Averaging.

Warum ist GBDT für Regressionsprobleme bevorzugt? Erstens erfasst es nichtlineare Beziehungen zwischen Features und Zielvariable, was lineare Modelle nicht können. Zweitens funktioniert es auf moderaten Datenmengen gut, im Gegensatz zu neuronalen Netzen. Drittens ist es vergleichsweise schnell zu trainieren. Viertens bietet es Feature-Importance-Schätzungen, die zeigen, welche Telemetrie-Signale am meisten zur Fahrzeugbalance-Vorhersage beitragen.

Zwei prominente Implementierungen konkurrieren: XGBoost (eXtreme Gradient Boosting) ist die ältere Implementierung, seit 2014 verfügbar, und bietet hochoptimierte Algorithmen mit integrierten Regularisierungsmechanismen wie L1- und L2-Penalisierungen, die Overfitting kontrollieren.\footnote{Vgl. Chen, Guestrin 2016, S. 785-794} Sie nutzt Histogram-basiertes Split-Finding: Statt Splits für alle kontinuierlichen Werte zu evaluieren, diskretisiert XGBoost Features in Histogramm-Bins, was zu Geschwindigkeitsvorteil führt. Sie hat native Unterstützung für kategoriale Features, wodurch manuelle Encoding-Schritte entfallen. XGBoost wächst Bäume level-wise: Bei jedem Schritt werden alle Blätter auf derselben Tiefe erweitert.

LightGBM (Light Gradient Boosting Machine) ist eine neuere Implementierung von Microsoft ab 2016, die speziell für Geschwindigkeit auf großen Datensätzen optimiert ist.\footnote{Vgl. Ke et al. 2017, S. 3146-3154} Sie verwendet Leaf-wise Wachstum: Statt alle Blätter auf gleicher Tiefe zu halten, erweitert sie bei jedem Schritt das Blatt mit dem höchsten Fehlerreduktionspotenzial, was zu tieferen, schmäleren Bäumen führt. Sie nutzt Gradient-based One-Side Sampling: Trainingsinstanzen mit großen Gradienten, die schlecht vorhergesagten, behalten volle Gewichtung, während Instanzen mit kleinen Gradienten, die gut vorhergesagten, untersampled werden. Sie nutzt Exclusive Feature Bundling: Hochkorrelierte Features werden kombiniert, um Dimensionalität zu reduzieren. Diese Optimierungen machen LightGBM oft 5 bis 10 mal schneller, zum Preis leicht erhöhten Overfitting-Risikos.

Empirisch zeigen beide Implementierungen überlegene Performance auf strukturierten tabularen Daten im Vergleich zu tiefen neuronalen Netzen.\footnote{Vgl. Chen, Guestrin 2016, S. 788-792} Die Wahl zwischen ihnen ist oft pragmatisch: XGBoost für Balance, LightGBM wenn Geschwindigkeit kritisch ist.



\section{Hyperparameter-Optimierung und Modellvalidierung}

Um ein GBDT-Modell zu trainieren, müssen vor dem Training viele Hyperparameter gesetzt werden. Hyperparameter unterscheiden sich fundamental von Modell-Parametern, den internen Gewichten des Modells, die während Training gelernt werden. Hyperparameter sind konfigurierbare Knöpfe, die Architekt oder Architektin des Modells kontrolliert.\footnote{Vgl. Bergstra, Bengio 2012, S. 281-305}

Für GBDT sind zentrale Hyperparameter die Anzahl der Estimators, also wie viele Bäume insgesamt trainiert werden, die Learning Rate als Schrittweite beim Gradient Descent, wobei kleine Werte konservativ sind und große Werte schneller, die Max Depth als maximale Tiefe jedes Baums mit tieferen Bäumen als komplexer, Min Child Weight als Mindestanzahl Samples in Blattknoten, wobei höhere Werte Overfitting verhindert, Subsample als Anteil Trainingsinstanzen pro Baum etwa 0,8 gleich 80 Prozent, und Regularisierungsparameter wie L2-Penalisierung.\footnote{Vgl. Neptune 2025, S. 1-20}

Die entscheidende Frage ist: Wie findet man die besten Hyperparameter? Manuelle Anpassung ist ineffizient. Stattdessen nutzt man Hyperparameter-Optimierung. Grid Search definiert ein vorgegebenes Gitter von Parameterwerten und probiert alle Kombinationen systematisch durch.\footnote{Vgl. Bergstra, Bengio 2012, S. 281-305} Vorteil ist eine garantiert gründliche Suche. Nachteil ist exponentieller Rechenaufwand mit Anzahl der Parameter.\footnote{Vgl. Neptune 2025, S. 1-20} Random Search probiert zufällig gewählte Kombinationen und kann effizienter sein, weil es den Parameterraum breiter abtastet.\footnote{Vgl. Bergstra, Bengio 2012, S. 281-305} Bayesian Optimization modelliert die Beziehung zwischen Hyperparametern und Performance wahrscheinlichkeitstheoretisch und sucht gezielt vielversprechende Regionen, was zu besseren Ergebnissen mit weniger Evaluationen führt.\footnote{Vgl. Snoek et al. 2012, S. 1-9}

Nach der Hyperparameter-Optimierung entsteht eine zweite Frage: Generalisiert das Modell auf unbekannte Daten? Diese Frage wird durch Modellvalidierung beantwortet. k-fold Cross-Validation ist der Standard: Der Datensatz wird in k gleiche Teile partitioniert. Das Modell wird k-mal trainiert, jeweils mit k minus eins Teilen als Training und 1 Teil als Validierung. Die k Validierungs-Scores werden gemittelt.\footnote{Vgl. Kohavi 1995, S. 1137-1145} Dies gibt eine robuste Schätzung der Generalisierungsperformance mit geringerer Varianz als ein einzelner Train-Test-Split.

Für Zeitreihendaten wie Telemetrie ist Standard-Cross-Validation aber problematisch: Sie blickt rückwärts, das heißt benutzt zukünftige Daten zum Trainieren und vergangende zum Test, was unrealistisch ist. Time Series Cross-Validation mit Forward Chaining simuliert stattdessen realistischere Szenarien: Das Modell wird auf frühe Wochen trainiert und auf späte Wochen validiert, dann wird das Fenster forward verschoben.\footnote{Vgl. Scaler 2023, S. 1-15} Dies respektiert die temporale Ordnung der Daten und vermeidet Datenlecks.

Im vorliegenden Projekt ist jedoch die kritischste Generalisierungsfrage: Transfer auf komplett neue Rennevents? Eine neue Event hat völlig andere Umgebung, andere Fahrer, anderes Setup. Standard-Cross-Validation löst dies nicht. Daher wird hier Leave-One-Event-Out verwendet: Das Modell wird auf 39 Veranstaltungen trainiert, dann auf die 40. Veranstaltung validiert, wiederholt für alle Veranstaltungen.\footnote{Vgl. Kapitel 4.1} Dies ist ein extremer, aber realistischer Stress-Test für echte Domänen-Generalisierung und offenbart, ob das Modell echte physikalische Strukturen gelernt hat oder nur Muster des Trainingsdatensatzes memoriert.

Diese drei Komponenten – Hyperparameter-Optimierung, Cross-Validation Strategie und Evaluationsmetrik – sind untrennbar: Zusammen stellen sie sicher, dass ein entwickeltes Modell tatsächlich verallgemeinerbar ist, nicht nur auf Trainingsdaten overfittet.


\section{Evaluationsmetriken für Regression}

Regressionsergebnisse werden durch mehrere etablierte Fehlermetriken quantifiziert. Der \ac{MSE} berechnet das Durchschnitt der quadrierten Abweichungen zwischen Vorhersagen und Ist-Werten und bestraft größere Fehler überproportional. Der \ac{RMSE} ist die Quadratwurzel des MSE und hat dieselbe Einheit wie die Zielvariable, was die Interpretation erleichtert.\footnote{Vgl. Hodson 2022, S. 5481-5482}

Der \ac{MAE} quantifiziert den Durchschnitt der absoluten Abweichungen und ist robuster gegenüber Ausreißern, da er Fehler linear (nicht quadratisch) gewichtet.\footnote{Vgl. Chai, Draxler 2014, S. 1247-1250} Die Wahl zwischen RMSE und MAE sollte sich nach der erwarteten Fehlerverteilung richten: RMSE ist optimal bei normalverteilten Fehlern, MAE bei Laplace-verteilten Fehlern.

Das Bestimmtheitsmaß R² (Coefficient of Determination) gibt an, welcher Anteil der Varianz der Zielvariable durch das Modell erklärt wird. R² = 1 signalisiert perfekte Vorhersagen, R² = 0 bedeutet, dass das Modell nicht besser als die Mittelwert-Baseline ist. Negative R²-Werte sind möglich und deuten auf schlechtere Performance als die Baseline hin.\footnote{Vgl. Hastie et al. 2009, S. 18-25} In der Ingenieurpraxis wird für Validierungsmodelle ein Schwellwert von R² $\geq$ 0,7 angestrebt, um praktische Einsatzfähigkeit zu gewährleisten.\footnote{Vgl. 365 Data Science 2023}

Diese drei Metriken (RMSE, MAE, R²) bilden den internationalen Standard in der Regressionsanalyse und ermöglichen Vergleichbarkeit mit etablierten Benchmarks in der Fachliteratur.