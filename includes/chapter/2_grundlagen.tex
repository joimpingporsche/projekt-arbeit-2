\chapter{Theoretische Grundlagen}

Zur Entwicklung eines \ac{ML}-Modells für Motorsport-Telemetrie-Analyse 
müssen mehrere ineinandergreifende Konzepte verstanden werden. Dieses Kapitel 
vermittelt zunächst methodologische Grundlagen (Design Science Research, 
Datenvorverarbeitung), dann die Theorie von Regressionsproblemen und speziell 
Gradient Boosting Decision Trees, und abschließend praktische Aspekte von 
Optimierung und Validierung. Diese Grundlagen bilden die wissenschaftliche Basis 
für die in Kapitel 4 beschriebene Artefakt-Entwicklung.


\section{Design Science Research Methodologie}

\ac{DSR} ist ein etablierter Forschungsansatz der Wirtschaftsinformatik, der sich grundlegend von deskriptiven Forschungsmethoden unterscheidet. Während traditionelle empirische Forschung primär darauf ausgerichtet ist, bestehende Phänomene zu verstehen und zu erklären, zielt \ac{DSR} darauf ab, praktische Probleme durch systematische Entwicklung und rigorose Evaluierung von Artefakten zu lösen. Damit schafft \ac{DSR} eine Brücke zwischen wissenschaftlicher Fundierung und praktischer Problemlösung.\footnote{Vgl. \cite{Hevner2004}, S. 77-83}

Das Grundwerk von Hevner et al. (2004) prägt bis heute das Verständnis von \ac{DSR} und etabliert ein Framework, das auf sieben präskriptiven Richtlinien basiert. Das Framework schreibt vor, dass \ac{DSR}-Projekte in drei ineinandergreifenden Zyklen durchgeführt werden sollten: Der Relevance Cycle beginnt mit Problemidentifikation aus der Anwendungsdomäne; der Rigor Cycle verankert die Entwicklung in wissenschaftlichem Wissen und etablierten Theorien; der Design Cycle orchestriert iterative Phasen von Artefakt-Konzeption, Entwicklung und Evaluierung. Diese drei Zyklen ermöglichen eine systematische und nachvollziehbare Forschungsvorgehensweise, bei der wissenschaftliche Strenge nicht auf Kosten von Praxisrelevanz geht.\footnote{Vgl. \cite{Hevner2004}, S. 77-92}

Der \ac{DSR}-Prozess gliedert sich typischerweise in sechs sequenzielle Phasen. Problem Identification and Motivation beginnt mit der Analyse der Problemdomäne und Begründung ihrer wissenschaftlichen und praktischen Relevanz. Definition of Objectives spezifiziert die Anforderungen, die das entwickelte Artefakt erfüllen muss. In der Design and Development Phase wird das Artefakt konzipiert und prototypisch implementiert. Die Demonstration Phase dokumentiert, dass das Artefakt das Problem tatsächlich lösen kann, typischerweise durch Fallstudien oder kontrollierte Szenarien. Die Evaluation Phase bewertet das Artefakt systematisch gegen die vordefinierten Anforderungen und Ziele. Abschließend erfolgt die Communication Phase, in der Erkenntnisse und Design Knowledge der wissenschaftlichen Gemeinschaft mitgeteilt werden.\footnote{Vgl. \cite{Peffers2007}, S. 45-77} 

Artefakte in \ac{DSR} können verschiedene Formen annehmen. Constructs sind konzeptionelle Vokabularien und Abstraktionen, die Probleme präzise definieren. Models stellen Zusammenhänge und Strukturen in vereinfachter Form dar. Methods sind Algorithmen und systematische Verfahrensweisen zur Problemlösung. Instantiations schließlich sind konkrete Implementierungen oder Prototypen.\footnote{Vgl. \cite{Hevner2004}, S. 80} In \ac{ML}-Projekten ist die Instantiation typischerweise ein trainiertes Modell mit vollständiger Pipeline (Datenvorverarbeitung, Feature Engineering, trainierte Parameter). Die vorliegende Arbeit entwickelt eine Instantiation: ein evaluiertes \ac{ML}-Modell für Fahrzeugbalance-Vorhersage.

Die Evaluierung in \ac{DSR} erfüllt mehrere funktionale Rollen. Sie stellt fest, ob und inwieweit das Artefakt die definierten Anforderungen erfüllt. Sie identifiziert Verbesserungspotenziale für weitere Iterationen. Vor allem trägt sie zur wissenschaftlichen Wissensbasis bei, indem Design Principles und generalisierbare Lessons Learned dokumentiert werden.\footnote{Vgl. \cite{Venable2016}, S. 79–81; vgl. dazu auch \cite{Gregor2007}, 
S. 322–325, sowie \cite{Hevner2004}, 
S. 84–85}
 Bewährte Evaluationsmethoden in \ac{DSR} sind observational (Feldbeobachtung), 
analytical (logische Deduktion und Proof-of-Concept), experimental 
(kontrollierte Experimente mit Baseline-Vergleich), testing (systematische 
Funktionsprüfung) und descriptive (qualitative Bewertung durch Experten).\footnote{Vgl. 
\cite{Hevner2004}, S. 85-86} Für \ac{ML}-Artefakte dominieren 
analytische und experimentelle Evaluationen mittels etablierter Performance-Metriken.\footnote{Vgl. \cite{Friedman2009}, S. 420–430}


\section{Experteninterviews zur Anforderungsermittlung}

Die Anforderungsanalyse in Kapitel 3 basiert auf zwei informellen Gesprächen mit einem erfahrenen Performance Engineer aus dem Porsche Motorsport-Team. Das erste Gespräch fand am 29.08.2025 statt, das zweite am 12.09.2025. Ziel war es, durch offene Diskussion die tägliche Arbeitsweise im Telemetrie-Management nachzuvollziehen, zentrale technische Herausforderungen zu identifizieren und realistische Anforderungen an ein automatisiertes Vorhersagemodell zu formulieren.
Der interviewte Experte agiert als Teamleiter der Performance-Abteilung im Porsche LMDh-Programm und ist verantwortlich für die Fahrzeugentwicklung, Simulationsmodellierung und insbesondere die datenbasierte Optimierung des Rennwagen-Setups während des operativen Betriebs, wodurch er über tiefgehendes, direkt anwendbares Wissen im Telemetrie-Management verfügt.\footnote{Vgl. Experteninterview 1, 29.08.2025, Z. 3-27}

Beide Gespräche folgten einem offenen, exploratorischen Format ohne strukturierten Fragenkatalog, um eine natürliche Konversation zu fördern und implizites Wissen des Experten zugänglich zu machen. Die gewonnenen Erkenntnisse bestätigten, dass die aktuelle Telemetrie-Analyse stark manuell erfolgt und mehrere Stunden pro Rennwochenende erfordert. Dies validierte die Problemrelevanz und leitete die Anforderungsableitung in Kapitel 3.\footnote{Vgl. Experteninterview 1 und 2, dokumentiert in Anhang A.1} 



\section{Maschinelle Lernverfahren für Regressionsprobleme}

\ac{ML} wird allgemein als automatische Induktion von vorhersagenden Modellen aus Daten definiert, ohne dass Algorithmen explizit programmiert werden müssen.\footnote{Vgl. Mitchell 1997, S. 1-2} Im praktischen Kontext bedeutet dies: Ein Lernalgorithmus erhält Trainingsdaten, erkennt Muster in diesen Daten und extrahiert eine verallgemeinerbare mathematische Struktur, die auf neue Daten angewendet werden kann.

Das vorliegende Projekt verfolgt ein Regressionsziel: Vorhersage einer kontinuierlichen Zielvariable (Fahrzeugbalance-Wert) aus einer Menge strukturierter Input-Features (Telemetrie-Metriken). Dies unterscheidet sich von Klassifikation, bei der diskrete Kategorien vorhergesagt werden. Regression ist ein Supervised-Learning-Problem: Jede Trainingsinstanz hat ein bekanntes, korrektes Label (den gemessenen Fahrzeugbalance-Wert), gegen das das Modell seine Vorhersagen abgleichen kann.\footnote{Vgl. Hastie et al. 2009, S. 1-25}

Das zentrale Problem beim Modelllernen wird durch den Bias-Variance Trade-off beschrieben. Ein Modell mit niedriger Komplexität wie lineare Regression hat hohen Bias: Es macht systematisch vereinfachte Vorhersagen, die die tatsächliche nichtlineare Beziehung zwischen Features und Zielvariable nicht erfassen und somit zu Underfitting führen. Umgekehrt weist ein hochkomplexes Modell wie ein überparametrisiertes Polynom hohe Varianz auf: Es memoriert Trainingsrauschen und besondere Trainingsfälle und generalisiert dadurch schlecht auf neue Daten, was als Overfitting bezeichnet wird.\footnote{Vgl. Hastie et al. 2009, S. 23-31} Das Ziel ist ein gutes Gleichgewicht: Ein Modell mit ausreichender Komplexität, um die echte Struktur der Daten zu erfassen, aber nicht so komplex, dass es Zufallsrauschen memoriert.

Die Vorbereitung der Daten ist entscheidend für späteren Erfolg. Exploratory Data Analysis (EDA) untersucht zunächst die Rohverteilungen der Features, identifiziert Korrelationen zwischen Variablen und erkennt potenzielle Ausreißer oder Anomalien.\footnote{Vgl. \cite{Tukey1977}, S. 1–50} Data Cleaning adressiert praktische Datenqualitätsprobleme: fehlende Werte werden behandelt durch Imputation oder Ausschluss, anomale Messwerte werden gefiltert, und sachlogische Schwellwerte werden gesetzt, etwa der Ausschluss von Messungen mit unplausiblen Sensorwerten.\footnote{Vgl. \cite{Chapman2000}, S. 23–28}

Feature Engineering ist der kreative Schritt, in dem Rohdaten in aussagekräftige Prädiktoren transformiert werden. Feature Selection wählt von vielen möglichen Kandidaten diejenigen aus, die am meisten zur Vorhersage beitragen und reduziert dadurch Overfitting sowie Trainingszeit.\footnote{Vgl. Guyon, Elisseeff 2003, S. 1157-1182} Aggregation kombiniert hochkorrelierte Sensoren zu zusammengefassten Features, beispielsweise der Durchschnitt mehrerer Temperatur-Sensoren, um Rauschredundanz zu minimieren.\footnote{Vgl. Guyon, Elisseeff 2003, S. 1157-1182} Encoding wandelt kategoriale Variablen wie Track-Identifikatoren in numerische Repräsentationen um.\footnote{Vgl. \cite{Kuhn2019}, S. 139–170} Für Telemetriedaten ist Zeitreihen-Glättung wie Moving Averages essentiell: Sie reduziert hochfrequentes Sensorrauschen, ohne Trends zu zerstören, und ermöglicht es, echte physikalische Änderungen von Messfehlern zu unterscheiden.\footnote{Vgl. \cite{Box2015}, S. 25–45; vgl. dazu auch \cite{Cleveland1979}, S. 829–836}


Nach der Datenvorbereitung entsteht eine zentrale Designfrage: Welcher Regressionsalgorithmus ist am besten geeignet? Lineare Modelle wie Linear Regression, Ridge und Lasso bieten exzellente Interpretierbarkeit, können aber intrinsisch nur lineare Beziehungen erfassen. Interpretierbarkeit hierbei, dass nachvollzogen werden kann, welche Features wie stark die Vorhersage beeinflussen.\footnote{Vgl. Hastie et al. 2009, S. 58-85} Support Vector Regression nutzt mathematische Tricks, die sogenannten Kernel-Tricks, um implizit nichtlineare Feature-Transformationen durchzuführen, bleibt aber schwer zu interpretieren.\footnote{Vgl. Hastie et al. 2009, S. 290-310} Decision Trees sind intuitiv: Sie partitionieren den Feature-Raum sequenziell anhand scharfer Grenzen, etwa wenn Feature X größer als 5 ist, dann gehe linken Ast. Sie sind robust und können komplexe nichtlineare Muster erfassen, leiden aber unter Overfitting, da ein einzelner Baum Trainingsrauschen memoriert.\footnote{Vgl. Breiman 2001, S. 5-32} Random Forests verbessern einzelne Bäume durch Ensemble-Averaging: Viele verschiedene Bäume werden auf zufällig gestörten Datensätzen trainiert, und ihre Vorhersagen werden gemittelt, was Varianz reduziert und Generalisierung verbessert.\footnote{Vgl. Breiman 2001, S. 5-32} Boosting-Methoden folgen einem anderen Ensemble-Prinzip: Sie trainieren Bäume sequenziell, wobei jeder neue Baum systematisch die Fehler vorheriger Bäume korrigiert.\footnote{Vgl. Friedman 2001, S. 1189-1232} Neuronale Netze können beliebig komplexe Funktionen approximieren und funktionieren gut bei großen Datenmengen, erfordern aber typischerweise mehr Trainingsdaten als traditionelle \ac{ML}-Methoden und sind weniger interpretierbar.\footnote{Vgl. Goodfellow et al. 2016, S. 164-223}

Die Frage stellt sich: Welche Methode eignet sich für das vorliegende Motorsport-Problem mit strukturierten, tabulischen Daten von moderater Größe, in denen konkurrierende komplexe Effekte zwischen Features existieren und Interpretierbarkeit gewünscht ist, um Renningenieure zu unterstützen? Unter diesen Bedingungen haben sich Gradient Boosting Decision Trees als Methode der Wahl etabliert, weil sie nichtlineare Muster erfassen, auf moderaten Datenmengen gut funktionieren und ein gutes Gleichgewicht zwischen Vorhersagegenauigkeit und Interpretierbarkeit bieten.\footnote{Vgl. Chen, Guestrin 2016, S. 785-794}



\section{Gradient Boosting Decision Trees}

Gradient Boosting Decision Trees (GBDT) sind eine Ensemble-Methode, die sequenzielle schwache Lerner, das heißt einfache Decision Trees, zu einem starken Vorhersagemodell kombiniert.\footnote{Vgl. Friedman 2001, S. 1189-1232} Das Kernprinzip ist Residual Learning: Der erste Baum wird auf die Rohdaten trainiert. Der zweite Baum wird trainiert, um die Fehler des ersten Baums vorherzusagen, die sogenannten Residuen. Der dritte Baum korrigiert dann die kombinierten Fehler von Baum 1 und 2, und so weiter. Die finale Vorhersage ergibt sich aus einer gewichteten Summe aller Baum-Ausgaben.

Formalisiert wird dies durch Gradient Descent: Eine Loss-Funktion, typischerweise Mean Squared Error für Regression, quantifiziert, wie schlecht die Vorhersagen sind. Jeder neue Baum wird so konstruiert, dass er in Richtung des negativen Gradienten dieser Loss-Funktion läuft, also gezielt Fehler reduziert.\footnote{Vgl. Friedman 2001, S. 1200-1210} Dieser Gradient-Ansatz führt zu schnellerer Konvergenz als alternatives Ensemble-Averaging.

Warum ist GBDT für Regressionsprobleme bevorzugt? Erstens erfasst es nichtlineare Beziehungen zwischen Features und Zielvariable, was lineare Modelle nicht können. Zweitens funktioniert es auf moderaten Datenmengen gut, im Gegensatz zu neuronalen Netzen. Drittens ist es vergleichsweise schnell zu trainieren. Viertens bietet es Feature-Importance-Schätzungen, die zeigen, welche Telemetrie-Signale am meisten zur Fahrzeugbalance-Vorhersage beitragen.

Zwei prominente Implementierungen konkurrieren: XGBoost (eXtreme Gradient Boosting) ist die ältere Implementierung, seit 2014 verfügbar, und bietet hochoptimierte Algorithmen mit integrierten Regularisierungsmechanismen wie L1- und L2-Penalisierungen, die Overfitting kontrollieren.\footnote{Vgl. Chen, Guestrin 2016, S. 785-794} Sie nutzt Histogram-basiertes Split-Finding: Statt Splits für alle kontinuierlichen Werte zu evaluieren, diskretisiert XGBoost Features in Histogramm-Bins, was zu Geschwindigkeitsvorteil führt. Sie hat native Unterstützung für kategoriale Features, wodurch manuelle Encoding-Schritte entfallen. XGBoost wächst Bäume level-wise: Bei jedem Schritt werden alle Blätter auf derselben Tiefe erweitert.

LightGBM (Light Gradient Boosting Machine) ist eine neuere Implementierung von Microsoft ab 2016, die speziell für Geschwindigkeit auf großen Datensätzen optimiert ist.\footnote{Vgl. \cite{Ke2017}, S. 1-10} Sie verwendet Leaf-wise Wachstum: Statt alle Blätter auf gleicher Tiefe zu halten, erweitert sie bei jedem Schritt das Blatt mit dem höchsten Fehlerreduktionspotenzial, was zu tieferen, schmäleren Bäumen führt. Sie nutzt Gradient-based One-Side Sampling: Trainingsinstanzen mit großen Gradienten, die schlecht vorhergesagten, behalten volle Gewichtung, während Instanzen mit kleinen Gradienten, die gut vorhergesagten, untersampled werden. Sie nutzt Exclusive Feature Bundling: Hochkorrelierte Features werden kombiniert, um Dimensionalität zu reduzieren. Diese Optimierungen machen LightGBM oft schneller, zum Preis leicht erhöhten Overfitting-Risikos.

Empirisch zeigen beide Implementierungen überlegene Performance auf strukturierten tabularen Daten im Vergleich zu tiefen neuronalen Netzen.\footnote{Vgl. \cite{Chen2016}, S. 788-792} Die Wahl zwischen ihnen ist oft pragmatisch: XGBoost für Balance, LightGBM wenn Geschwindigkeit kritisch ist.



\section{Hyperparameter-Optimierung und Modellvalidierung}

Um ein GBDT-Modell zu trainieren, müssen vor dem Training viele Hyperparameter gesetzt werden. Hyperparameter unterscheiden sich fundamental von Modell-Parametern, den internen Gewichten des Modells, die während Training gelernt werden. Hyperparameter sind konfigurierbare Knöpfe, die Architekt oder Architektin des Modells kontrolliert.\footnote{Vgl. Bergstra, Bengio 2012, S. 281-305}

Für GBDT sind mehrere Hyperparameter besonders wichtig: Die Anzahl der Estimators bestimmt, wie viele Bäume insgesamt trainiert werden, während die Learning Rate als Schrittweite beim Gradientenverfahren fungiert – kleine Werte führen zu langsameren, stabileren Verbesserungen, große Werte zu schnellerem, aber riskanterem Lernen. Die Max Depth definiert die maximale Tiefe jedes Baums und erhöht mit steigenden Werten die Modellkomplexität. Das Min Child Weight (bzw. die Mindestanzahl an Samples pro Blatt) bewirkt bei höheren Werten konservativere Splits und damit weniger Overfitting, während Subsample den Anteil der Trainingsinstanzen pro Baum festlegt (z. B. 0{,}8 für 80\%). Schließlich kontrollieren Regularisierungsparameter wie L2-Penalisierung die Modellkomplexität.\footnote{Vgl. \cite{Chen2016}, S. 787–788}
Da manuelle Anpassung dieser Parameter ineffizient ist, nutzt man systematische Hyperparameter-Optimierung. Grid Search definiert ein vorgegebenes Gitter von Parameterwerten und probiert alle Kombinationen systematisch durch, garantiert zwar eine gründliche Suche, führt aber zu exponentiellem Rechenaufwand mit der Anzahl der Parameter.\footnote{Vgl. Bergstra, Bengio 2012, S. 281-305; vgl. dazu auch \cite{Chen2016}, S. 787–788}
Nach der Hyperparameter-Optimierung ergibt sich eine weitere zentrale Herausforderung: Generalisiert das Modell auf unbekannte Daten? Diese wird durch Modellvalidierung beantwortet. Der Standard k-fold Cross-Validation partitioniert den Datensatz in k gleiche Teile und trainiert das Modell k-mal, jeweils mit k minus eins Teilen als Training und einem Teil als Validierung. Die gemittelten k Validierungs-Scores geben eine robuste Schätzung der Generalisierungsperformance mit geringerer Varianz als ein einzelner Train-Test-Split.\footnote{Vgl. Kohavi 1995, S. 1137-1145}

Im vorliegenden Projekt ist jedoch die kritischste Generalisierungsfrage der Transfer auf komplett neue Rennevents, da eine neue Veranstaltung völlig andere Umgebung, andere Fahrer und anderes Setup aufweist. Standard-Cross-Validation löst diese Herausforderung nicht, weshalb hier Leave-One-Out verwendet wird. Dafür wird das Modell auf einem Trainingsdatensatz trainiert, der alle Veranstaltungen außer einer enthält, und dann auf der zurückgehaltenen Veranstaltung validiert.\footnote{Vgl. \cite{James2021}, S. 176-195} Dies stellt einen extremen, aber realistischen Stress-Test für echte Domänen-Generalisierung dar und offenbart, ob das Modell echte physikalische Strukturen gelernt hat oder nur Muster des Trainingsdatensatzes memoriert hat.

Diese drei Komponenten Hyperparameter-Optimierung, Cross-Validation Strategie und Evaluationsmetrik sind untrennbar: Zusammen stellen sie sicher, dass ein entwickeltes Modell tatsächlich verallgemeinerbar ist, nicht nur auf Trainingsdaten overfittet.


\section{Evaluationsmetriken für Regression}

Regressionsergebnisse werden durch mehrere etablierte Fehlermetriken quantifiziert. Der \ac{MSE} berechnet das Durchschnitt der quadrierten Abweichungen zwischen Vorhersagen und Ist-Werten und bestraft größere Fehler überproportional. Der \ac{RMSE} ist die Quadratwurzel des MSE und hat dieselbe Einheit wie die Zielvariable, was die Interpretation erleichtert.\footnote{Vgl. Hodson 2022, S. 5481-5482}

Der \ac{MAE} quantifiziert den Durchschnitt der absoluten Abweichungen und ist robuster gegenüber Ausreißern, da er Fehler linear (nicht quadratisch) gewichtet.\footnote{Vgl. Chai, Draxler 2014, S. 1247-1250} Die Wahl zwischen RMSE und MAE sollte sich nach der erwarteten Fehlerverteilung richten: RMSE ist optimal bei normalverteilten Fehlern, MAE bei Laplace-verteilten Fehlern.

Das Bestimmtheitsmaß R² (Coefficient of Determination) gibt an, welcher Anteil der Varianz der Zielvariable durch das Modell erklärt wird. R² = 1 signalisiert perfekte Vorhersagen, R² = 0 bedeutet, dass das Modell nicht besser als die Mittelwert-Baseline ist. Negative R²-Werte sind möglich und deuten auf schlechtere Performance als die Baseline hin.\footnote{Vgl. Hastie et al. 2009, S. 10-60} In der Ingenieurpraxis und insbesondere im Motorsport-Datenkontext werden für 
Validierungsmodelle üblicherweise Schwellwerte von R² $\geq$ 0,7 angestrebt, 
da dies eine Fehlerreduktion von etwa 50\% gegenüber einem Baseline-Modell 
bedeutet und damit praktische Einsatzfähigkeit gewährleistet.\footnote{Vgl. \cite{ODonnell2024}, S. 1-10}


Diese drei Metriken (RMSE, MAE, R²) bilden den internationalen Standard in der Regressionsanalyse und ermöglichen Vergleichbarkeit mit etablierten Benchmarks in der Fachliteratur.