\chapter{Evaluation und Interpretation des Vorhersagemodells}

Die vorangegangenen Kapitel dokumentierten die systematische Entwicklung von 128 Modellkonfigurationen basierend auf 16 Datensatzstrukturen nach der Design Science Research Methodik. Das vorliegende Kapitel bildet die Evaluate- und Reflect-Phase des Design Cycle und wertet die Modellperformance systematisch aus sowie interpretiert die Ursachen identifizierter Leistungsgrenzen. Im Kontext der DSR-Methodik müssen drei zentrale Dimensionen nachgewiesen werden: \textit{Utility} (der praktische Nutzen des Modells), \textit{Quality} (die Robustheit und Zuverlässigkeit) sowie \textit{Efficacy} (die Problemlösung und Anforderungserfüllung).\footnote{Vgl. \cite{Venable2016}, S. 79-82}

Die Evaluation erfolgt primär als artificial Evaluation durch quantitative Metriken auf strukturkonsistenten Validierungsdatensätzen.\footnote{Vgl. \cite{Venable2016}, S. 80-87} Eine naturalistic Evaluation durch Expertenfeedback wird bewusst nicht in diesem Kapitel durchgeführt, da die Modellperformance auf Event-Validierungsdatensätzen bereits zeigt, dass das Artefakt nicht für produktiven Einsatz geeignet ist. Stattdessen fokussiert die Analyse auf die technische Ursachenforschung, um generalisierbares Design Knowledge für zukünftige \ac{ML}-Projekte im Motorsport abzuleiten. Die anschließende Interpretation der Evaluationsergebnisse identifiziert Wurzelursachen beobachteter Leistungsgrenzen und schließt damit den Rigor Cycle der DSR-Methodik.


\section{Evaluationskonzept und -methodik}

Die Evaluationsstrategie folgt dem FEDS-Framework und adressiert vier zentrale Fragen: \\ \textbf{Warum} erfolgt die Evaluation (summativ: abschließende Qualitätsbewertung des Artefakts), \textbf{wann} (ex-post nach Modelltraining und -optimierung), \textbf{wie} (kombiniert quantitativ und qualitativ, naturalistisch im Praxiskontext) und \textbf{was} wird evaluiert (Prädiktionsgenauigkeit, algorithmische Eigenschaften, Datensatzstruktur-Effekte).\footnote{Vgl. \cite{Venable2016}, S. 77-89}

Für alle 256 Evaluationsergebnisse (128 Modelle × 2 Validierungsstrategien je Datensatzstruktur) werden die standardisierten Regressionsmetriken $R^2$, MAE und RMSE verwendet, deren Definitionen bereits in Kapitel 2 erläutert wurden.

Diese drei Metriken folgen etablierten Standards in der \ac{ML}-Literatur.\footnote{Vgl. \cite{Hodson2022}, S. 5481–5492; vgl. dazu auch \cite{Willmott2005}, S. 79–82}

Die Evaluation untersucht systematisch (i) die Prädiktionsgenauigkeit auf beiden Validierungs-\\datensatz-Typen (Generalisierung vs. Robustheit), (ii) den Vergleich zwischen LightGBM und XGBoost, (iii) die Effekte der vier Hyperparameter-Komplexitätsstufen und (iv) die Auswirkungen von Glättung, Feature-Reduktion und kategorialen Features auf die Performance.


\section{Quantitative Leistungsanalyse}

Die Evaluation der 128 trainierten Modelle auf zwei strukturkonsistenten Validierungsdatensätzen zeigt eine stark zweigeteilte Leistungslandschaft, die erhebliche Implikationen für die praktische Einsatzfähigkeit des Artefakts hat. Im Folgenden werden zunächst die positiven Ergebnisse auf dem Zufalls-Validierungsdatensatz dargestellt, anschließend die kritischen Befunde der Event-basierten Validierung erörtert und abschließend die Generalisierungsproblematik analysiert.
Die kompletten Ergebnisse der Validierung sind im Anhang \ref{Modell-Ergebnisse} dokumentiert.

\subsection{Performance auf Zufalls-Validierungsdatensätzen}

Die Zufalls-Validierung gibt Aufschluss über die Modellqualität auf typischen, aber ungesehenen Daten innerhalb der trainierten Verteilung. Die Ergebnisse sind substantiell positiv und zeigen, dass das entwickelte Artefakt unter Standardbedingungen akzeptable Vorhersagefähigkeit aufweist.
Durchschnittlich erreicht das Modellportfolio eine R² von $0.308$ (±$0.197$), mit einer Range von $-0.338$ bis $0.657$. Dies bedeutet, dass das durchschnittliche Modell etwa ein Drittel der Varianz in der Zielvariable erklären kann. Die mittlere RMSE-Abweichung liegt bei $0.328$ (±$0.049$), was einer durchschnittlichen Vorhersageabweichung von etwa $0.33$ Understeer-Einheiten entspricht.\footnote{Vgl. \cite{Hodson2022}, S. 5481-5482} Damit liegen die Modelle in der praktischen Größenordnung, die für Engineering-Fragestellungen relevant ist. XGBoost dominiert deutlich die Random-Validierungsperformance mit einem Durchschnitts-R² von $0.380$ (±$0.235$), während LightGBM mit $0.237$ (±$0.112$) deutlich dahinter liegt. XGBoost zeigt auch überlegene Stabilität mit geringerer Standardabweichung.

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Algorithmus} & \textbf{R\textsuperscript{2} (Mittel)} & \textbf{MAE (Mittel)}  & \textbf{RMSE (Mittel)} & \textbf{Anzahl} \\
    \midrule
    XGBoost & 0.380 & 0.239 & 0.308 & 64 \\
    LightGBM & 0.237 & 0.278 & 0.347 & 64 \\
    \midrule
    \textbf{Vorteil XGB} & \textbf{+0.143} & \textbf{-0.038} & \textbf{-0.039} & \textbf{--} \\
    \bottomrule
  \end{tabular}
  \caption{Algorithmus-Vergleich: Random-Validierung (alle Komplexitätsstufen)}
  \label{tab:algo_random}
\end{table}

Die beste Modell-Konfiguration in der Random-Validierung ist ein XGBoost-Modell mit Very Deep-Komplexität, ohne kategoriale Features, mit Feature-Aggregation und ohne Zielvariablen-Glättung. Dieses Modell erreicht ein $R^2$ von $0{,}657$, RMSE von $0{,}233$ und MAE von $0{,}176$. Dies deutet darauf hin, dass XGBoost unter Bedingungen mit ausreichender Komplexität und geeigneter Feature-Konfiguration starke Vorhersagefähigkeit entwickelt. Bemerkenswerterweise ist die mittlere Komplexität nicht universell optimal. Medium und Very Deep erzielen ähnlich gute Durchschnittswerte (R² von $0.336$ bzw. $0.328$), während Shallow mit $0.245$ deutlich schlechter abschneidet. Dies deutet darauf hin, dass eine Mindest-Modellkapazität erforderlich ist, dass aber zu tiefe Modelle auf Random-Daten nicht zusätzlich helfen.

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
  	\textbf{Konfiguration} & \textbf{R\textsuperscript{2} (Mittel)} & \textbf{MAE (Mittel)}  & \textbf{RMSE (Mittel)} & \textbf{Anzahl} \\
    \midrule
    Shallow & 0.245 & 0.220 & 0.343 & 32 \\
    Medium & 0.336 & 0.183 & 0.322 & 32 \\
    Deep & 0.325 & 0.191 & 0.324 & 32 \\
    Very Deep & 0.328 & 0.186 & 0.323 & 32 \\
    \bottomrule
  \end{tabular}
  \caption{Random-Validierung: Performance nach Hyperparameter-Komplexitätsstufe}
  \label{tab:complexity_random}
\end{table}

Die Glättungsvarianten zeigen eine optimale Performance bei Fenstergrößen von 2–3, mit Fenstergröße 3 leicht vorne (R² $0.324$). Fenstergröße 4 verschlechtert die Performance (R² $0.297$). Feature-Aggregation liefert einen konsistenten, wenn auch moderaten Vorteil ($+0.024$ R²). Überraschenderweise profitieren Random-Validierungsergebnisse deutlich von der Abwesenheit kategorialer Features: Modelle ohne kategoriale Features erreichen R² $0.466$ gegenüber R² $0.150$ mit kategorialen Features eine Differenz von 0.316. Dies ist ein starker Indikator für Overfitting auf kategoriale Variablen.

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Konfiguration} & \textbf{R² (Mittel)} & \textbf{Varianz} & \textbf{RMSE (Mittel)} & \textbf{Modelle} \\
    \midrule
    \multicolumn{5}{l}{\textbf{Kategoriale Features:}} \\
    Ohne kategorische Features & 0.466 & 0.195 & 0.289 & 64 \\
    Mit kategorialen Features & 0.150 & 0.080 & 0.367 & 64 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Feature-Aggregation:}} \\
    Ohne Aggregation & 0.296 & 0.183 & 0.331 & 64 \\
    Mit Aggregation & 0.320 & 0.210 & 0.325 & 64 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Glättung (Fenstergrößen):}} \\
    Fenster 0 (keine) & 0.309 & 0.195 & 0.328 & 32 \\
    Fenster 2 & 0.304 & 0.215 & 0.329 & 32 \\
    Fenster 3 & 0.324 & 0.186 & 0.325 & 32 \\
    Fenster 4 & 0.297 & 0.197 & 0.331 & 32 \\
    \bottomrule
  \end{tabular}
  \caption{Random-Validierung: Effekte der Datensatzstruktur-Varianten}
  \label{tab:structure_random}
\end{table}

Auf dem Zufalls-Validierungsdatensatz demonstriert das beste Modell (XGBoost Very Deep, $R^2 = 0.657$) starke Vorhersagefähigkeit. Der Durchschnitt über alle Modelle liegt bei akzeptablen R² $0.308$. Diese Befunde suggerieren, dass das entwickelte Artefakt unter bekannten Datenverteilungen verlässliche Prognosen liefert.


\subsection{Performance auf Event-Validierungsdatensätzen}

Die Event-basierte Validierung wendet das trainierte Modell auf ein vollständig unbekanntes Renn-Event an und prüft damit die echte Generalisierungsfähigkeit auf neue Rennkontexte, Fahrzeugkonfigurationen und Streckeneigenschaften. Die Befunde in diesem Szenario sind fundamental kritisch. Die durchschnittliche R² beträgt $-0.306$ (±$0.256$), eine deutlich negative Zahl. Dies bedeutet, dass das durchschnittliche Modell schlechter abschneidet als eine triviale Baseline (z.B. Mittelwert-Vorhersage). Die Range erstreckt sich von $-1.135$ bis $0.093$, wobei 25 von 128 Modellen ein R² unter $-0.5$ aufweisen, ein Indikator extremer Fehlvorhersagen. Zusätzlich beträgt die RMSE durchschnittlich $0.325$ (±$0.032$), was zwar der Random-Validierung ähnelt, aber auf Grund der negativen R²-Werte nicht aussagekräftig ist. LightGBM zeigt eine überlegene Event-Generalisierbarkeit mit R² $-0.268$ (±$0.277$) gegenüber XGBoost mit $-0.343$ (±$0.229$). Der beste LightGBM-Event-Score (R² $0.093$) ist deutlich höher als der beste XGBoost-Event-Score (R² $0.029$).

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Algorithmus} & \textbf{R² (Mittel)} & \textbf{R² (Best)} & \textbf{RMSE (Mittel)} & \textbf{Modelle} \\
    \midrule
    LightGBM & -0.268 & 0.093 & 0.320 & 64 \\
    XGBoost & -0.343 & 0.029 & 0.330 & 64 \\
    \midrule
    \textbf{Vorteil LGB} & \textbf{+0.075} & \textbf{+0.064} & \textbf{-0.010} & -- \\
    \bottomrule
  \end{tabular}
  \caption{Algorithmus-Vergleich: Event-Validierung (Leave-One-Out)}
  \label{tab:algo_event}
\end{table}

Die beste Modell-Konfiguration in der Event-Validierung ist ein LightGBM-Modell mit Very Deep-Komplexität, mit kategorialen Features, ohne Feature-Aggregation und ohne Zielvariablen-Glättung. Dieses Modell erreicht R² $0.093$, RMSE $0.272$ und MAE $0.212$. Auch diese beste Konfiguration bleibt problematisch niedrig.

Tiefere Modelle zeigen relativ bessere Event-Generalisierung: Very Deep (R² $-0.232$) übertrifft Shallow (R² $-0.405$) um $0.173$ Punkte. Dies deutet darauf hin, dass höhere Modellkomplexität das Generalisierungsproblem nicht verschärft. Allerdings sind \textbf{alle Komplexitätsstufen im absoluten Sinne unzureichend}. Der beobachtete Trend sollte daher nicht als Beleg für robustere Modelle interpretiert werden, sondern lediglich als Hinweis, dass Overfitting durch Komplexität in diesem Fall nicht der limitierende Faktor ist.

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Komplexitätsstufe} & \textbf{R² (Mittel)} & \textbf{Varianz} & \textbf{RMSE (Mittel)} & \textbf{Modelle} \\
    \midrule
    Shallow & -0.405 & 0.317 & 0.337 & 32 \\
    Medium & -0.318 & 0.233 & 0.327 & 32 \\
    Deep & -0.267 & 0.235 & 0.321 & 32 \\
    Very Deep & -0.232 & 0.204 & 0.317 & 32 \\
    \bottomrule
  \end{tabular}
  \caption{Event-Validierung: Performance nach Komplexitätsstufe (Trend zu besserer Generalisierung)}
  \label{tab:complexity_event}
\end{table}

% TODO: Abbildung 5.2.3 - Linie/Punkte-Diagramm: Komplexität vs. R² Event

Bei der Event-Generalisierung zeigt Glättung (Fenster 3: R² $-0.281$) einen marginalen Vorteil gegenüber keiner Glättung (R² $-0.330$). Kategoriale Features sind ambivalent. Sie verbessern Event-Generalisierung erheblich (mit: R² $-0.140$ vs. ohne: R² $-0.471$, Delta-R² $+0.331$), schaden aber massiv in der Random-Validierung (mit: R² $0.150$ vs. ohne: R² $0.466$, Delta-R² $-0.316$).

\begin{table}[H]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Konfiguration} & \textbf{R² (Mittel)} & \textbf{Varianz} & \textbf{RMSE (Mittel)} & \textbf{Modelle} \\
    \midrule
    \multicolumn{5}{l}{\textbf{Kategoriale Features:}} \\
    Mit kategorialen Features & -0.140 & 0.207 & 0.304 & 64 \\
    Ohne kategorische Features & -0.471 & 0.184 & 0.346 & 64 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Feature-Aggregation:}} \\
    Mit Aggregation & -0.300 & 0.248 & 0.325 & 64 \\
    Ohne Aggregation & -0.311 & 0.266 & 0.326 & 64 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Glättung (Fenstergrößen):}} \\
    Fenster 0 (keine) & -0.330 & 0.316 & 0.328 & 32 \\
    Fenster 2 & -0.296 & 0.259 & 0.324 & 32 \\
    Fenster 3 & -0.281 & 0.219 & 0.323 & 32 \\
    Fenster 4 & -0.315 & 0.229 & 0.327 & 32 \\
    \bottomrule
  \end{tabular}
  \caption{Event-Validierung: Effekte der Datensatzstruktur-Varianten}
  \label{tab:structure_event}
\end{table}

Die Event-Validierung offenbart ein fundamentales Generalisierungsproblem. Mit durchschnittlich negativer R² sind die Modelle in diesem Szenario nicht praktisch einsetzbar. Das beste Modell mit R² $0.093$ ist gerade noch marginale besser als eine Baseline.

\subsection{Interpretation des Gesamtergebnisses}

Die massive Diskrepanz zwischen Random- (R²-Mittel $0.308$) und Event-Validierung (R²-Mittel $-0.306$) ist nicht auf typisches Overfitting zurückzuführen, sondern offenbart ein fundamentales Problem der Domänen-Generalisierung. Im Folgenden werden die wahrscheinlichen Ursachen und Implikationen dieser Diskrepanz erörtert.

Ein zentrales Problem besteht darin, dass die Telemetriedaten ausschließlich Fahrzeugzustände und Sensorwerte erfassen, jedoch die entscheidenden Kontextfaktoren, die das Understeer maßgeblich beeinflussen, nicht berücksichtigen. So sind beispielsweise individuelle Fahrereffekte wie Lenk-Aggression, Bremspunkt-Variabilität oder die Wahl der Fahrlinie in den Rohdaten mit physikalischen Fahrzeug-Eigenschaften vermischt und lassen sich nicht voneinander trennen. Das Modell erlernt dadurch eine Kombination aus Fahrzeugverhalten und Fahrercharakteristik, die bei neuen Fahrern nicht übertragbar ist. Hinzu kommt, dass das Fahrzeug-Setup, etwa Chassis-Steifigkeit, Aero-Balance oder Federung, sich zwischen den Events und sogar innerhalb eines Events verändert. Diese Parameter sind in den Telemetriedaten nicht explizit enthalten und können allenfalls indirekt über die Reaktion des Fahrzeugs auf die Fahrbahn abgeleitet werden, was jedoch eine unzureichende und wenig robuste Methode darstellt. Darüber hinaus spielen Umgebungsfaktoren wie Streckentemperatur, Luftdichte, Feuchtigkeit und Windverhältnisse eine fundamentale Rolle für Aerodynamik und Reifenverhalten. Da diese Einflussgrößen im Datensatz nicht enthalten sind, können sie auch nicht rekonstruiert werden. Insgesamt erklärt das Fehlen dieser Kontextfaktoren, weshalb das Modell zwar auf bekannten Daten eine hohe Performance erzielt, jedoch beim Transfer auf neue Events deutlich an Vorhersagekraft verliert.

Diese fehlenden Faktoren erklären, warum das Modell eine hohe Random-Validierungs-Performance erreicht (es memoriert Training-Event-Muster) aber bei Event-Transfer kollabiert (neue Kombinationen aus Fahrer, Setup, Reifen, Umwelt sind unbekannt).

Das zentrale Problem ist ein Covariate-Shift: Die Verteilung der Eingabedaten unterscheidet sich zwischen Training und Event-Test, während das Modell unter der Annahme konstanter Kontextfaktoren trainiert wurde. Bei neuen Fahrern, Setups oder Umwelteinflüssen versagt diese Annahme, und die Generalisierung bricht ein. Der Event-Validierungsdatensatz dient somit als Stress-Test für echte Domain Generalization. Für zukünftige \ac{ML}-Projekte gilt: Die Datenqualität ist entscheidender als die Wahl des Algorithmus, und die Verwendung kategorialer Features sollte mit Blick auf die Ziel-Domäne erfolgen. Robustere Generalisierung erfordert gezieltes Feature-Engineering und Transfer-Learning-Ansätze, da Modelle ohne explizite Berücksichtigung neuer Kontexte nicht zuverlässig auf unbekannte Strecken übertragbar sind.
