\chapter{Fazit, Erkenntnisse und Forschungsausblick}
Mit Kapitel 5 wurde die technische Evaluierung des Vorhersagemodells abgeschlossen. Das vorliegende Kapitel schließt den Design Cycle durch Reflexion, Synthese und Kommunikation ab. Dabei werden die empirischen Ergebnisse der Evaluierung explizit gegen die in Kapitel 3 formulierten Anforderungen und Forschungsfragen abgewogen. Zentral ist die kritische Analyse der identifizierten Leistungsgrenzen. Abschließend werden generalisierbare Design Principles und Erkenntnisse für zukünftige ML-Projekte im Motorsport-Kontext abgeleitet, wodurch diese Arbeit über das konkrete Artefakt hinaus einen Beitrag zur Wissensbasis leistet.

\section{Erfüllung der Anforderungen und Beantwortung der Forschungsfragen}

Die vorliegende Arbeit verfolgte das Ziel, ein \ac{ML}-Modell zur automatisierten Vorhersage von Understeer-Verhalten in Motorsport-Telemetriedaten zu entwickeln. Zur Bewertung des Erfolgs werden sowohl die technischen Anforderungen als auch die formulierten Forschungsfragen gegen die erhaltenen Ergebnisse abgewogen.

\textbf{Anforderung 1: Automatisierte Understeer-Vorhersage}

Die Anforderung, ein \ac{ML}-Modell zur Vorhersage von Understeer-Werten auf Basis von Telemetriedaten zu entwickeln, ist partiell erfüllt. Das beste Modell auf dem Zufalls-Validierungs-\\datensatz (XGBoost, Very Deep, R² = 0.657) demonstriert, dass Understeer-Vorhersage unter kontrollierten Bedingungen möglich ist. Die durchschnittliche Performance auf dem Zufallsdatensatz über alle 128 Modelle beträgt R² = 0.308, was darauf hindeutet, dass die Modelle etwa ein Drittel der Varianz in der Zielvariablen erfassen. Allerdings ist diese Performance auf die spezifische Datenverteilung des Trainingsdatensatzes beschränkt. Bei der Event-Validierung, dem realistischeren Szenario mit vollständig unbekannten Rennkontexten, kollabiert die Performance dramatisch. Das beste Event-Modell (LightGBM, Very Deep, R² = 0.093) erreicht nur marginal bessere Ergebnisse als eine Baseline-Vorhersage, und der Durchschnitt über alle Modelle beträgt R² = -0.306, was bedeutet, dass die Modelle schlechter abschneiden als triviale Vergleichsmodelle. Diese Diskrepanz offenbart, dass das Ziel einer produktionsreifen, generalisierbaren Vorhersage nicht erreicht wurde.

\textbf{Anforderung 2: Hohe Vorhersagegenauigkeit}

Die angestrebte Vorhersagegenauigkeit von R² > 0,7 wurde nicht erreicht. Das beste Zufalls-Modell erreicht R² = 0,657, knapp unterhalb der Zielvorgabe, scheitert aber bei Event-Generali-\\sierung (R² = 0,093). Die Ursachenanalyse in Kapitel 5.2.3 identifiziert als Primärproblem nicht algorithmische Limitationen, sondern fehlende Kontextfaktoren: Fahrercharakteristiken, Setup-Parameter, Reifen-Degradation und Umgebungsdaten sind in den verfügbaren Telemetriedaten nicht erfasst und können nicht aus Sensorwerten rekonstruiert werden. Diese fehlenden Variablen determinieren das Understeer-Verhalten fundamental und liefern eine mögliche Erklärung für die schwache Event-Generalisierung.



\section{Beantwortung der Forschungsfragen}

\textbf{Forschungsfrage 1: Können Gradient Boosting Decision Trees Understeer-Verhalten in Motorsport-Telemetriedaten vorhersagen?}

Die Antwort lautet: Teilweise ja, aber nur unter stark einschränkenden Bedingungen. GBDT-Modelle funktionieren auf trainierten Datensätzen gut (durchschnittliche Zufalls-Validierung: R² = 0,308) und zeigen damit, dass Understeer grundsätzlich aus Telemetriedaten extrahierbar ist. Allerdings generalisieren diese Modelle nicht auf unbekannte Rennkontexte (durchschnittliche Event-Validierung: R² = -0,306). Das Problem liegt nicht in der Algorithmuswahl, sondern in der fehlenden Datengrundlage. GBDT memorisieren Muster aus trainierten Events (Track-spezifische Telemetrie-Signaturen), können diese aber nicht auf neue Tracks, neue Fahrer*innen oder neue Setup-Konfigurationen übertragen.

\textbf{Forschungsfrage 2: Welcher Algorithmus (XGBoost vs. LightGBM) generalisiert besser?}

Weder XGBoost noch LightGBM zeigt konsistente Überlegenheit. Bei Event-Generalisierung ist LightGBM leicht besser (durchschnittliche R² = -0,268 vs. XGBoost -0,343), mit dem besten Event-Modell bei R² = 0,093 (LightGBM) vs. R² = 0,029 (XGBoost). Bei Zufalls-Validierung dominiert XGBoost deutlich (durchschnittliche R² = 0,380 vs. LightGBM 0,237), mit dem besten Modell bei R² = 0,657 (XGBoost) vs. R² = 0,458 (LightGBM). Diese gegensätzliche Leistung unterstreicht, dass algorithmische Wahl sekundär ist. Beide Algorithmen leiden unter denselben fundamentalen Limitationen der fehlende Kontextfaktoren.

\textbf{Forschungsfrage 3: Wie wirken sich Datenvorbereitung und Hyperparameter-Tuning auf die Performance aus?}

Der größte Effekt stammt nicht aus Hyperparameter-Tuning, sondern aus der Feature-Konfigura-\\tion, insbesondere der Verwendung kategorialer Features. In der Event-Validierung helfen kategoriale Features (primär Track-Information) um Delta-R² = +0,331 (mit Kategorisch: R² = -0,140 vs. ohne: R² = -0,471). In der Zufalls-Validierung schaden sie um Delta-R² = -0,316 (mit Kategorisch: R² = 0,150 vs. ohne: R² = 0,466). Glättungsvarianten zeigen moderate Effekte (optimal: Fenster 3, Delta-R² ca. 0,03–0,05), und Feature-Aggregation hat minimalen Effekt (Delta-R² < 0,03). Hyperparameter-Komplexitätsstufen zeigen, dass tiefere Modelle bei Event-Generalisierung helfen (Very Deep besser als Shallow um Delta-R² = +0,173), was gegen klassisches Overfitting-Verständnis spricht und eher auf Domain-Shift-Probleme hindeutet.



\section{Design Knowledge und kritische Selbsteinschätzung}

Bedauernswerterweise konnte das Ziel, ein \ac{ML}-Modell zur Vorhersage der Fahrzeugbalance zu entwickeln, nicht erreicht werden. Trotz intensiver Bemühungen in der Datensammlung (17.735 Runden aus 40 Events), Datenvorbereitung (16 Strukturvarianten) und Modellierung (128 trainierte Modelle) blieb die angestrebte Prognosegenauigkeit aus, sodass diese Arbeit kein in der Praxis nutzbares Artefakt hervorgebracht hat. Im Normalfall würden nun mehrere Iterationen der Cycles durchlaufen werden, was im Rahmen dieser Arbeit aufgrund von Zeit- und Ressourcenbeschränkungen nicht möglich war.\footnote{Vgl. \cite{Hevner2004}, S. 83}

Aus Sicht der Design Science Research Methodik ist die Evaluate-Phase vollständig dokumentiert: Der Rigor Cycle wurde mit systematischen Evaluationen, etablierten Metriken und 256 Evaluationsergebnissen erfüllt.\footnote{Vgl. \cite{Venable2016}, S. 77 ff.} Der Relevance Cycle wurde teilweise erfüllt und die zugrunde liegenden Probleme aufgezeigt, die Lösung erfordert aber Daten, die außerhalb des Projektumfangs liegen.

Aus den Erkenntnissen des Entwicklungsprozesses ergeben sich folgende Design Principles für zukünftige Arbeiten:
Die algorithmische Wahl (XGBoost vs. LightGBM) ist für die Vorhersagegenauigkeit nachrangig; entscheidend ist die Datenqualität und die Berücksichtigung relevanter Kontextfaktoren. Kategoriale Features wie Track-Informationen verbessern die Modellleistung innerhalb bekannter Datenbereiche, verschlechtern jedoch die Generalisierung auf neue Events, da sie lediglich Trainingsmuster memorisieren. Die Analyse zeigt, dass Domain Shift, also Unterschiede in der Verteilung der Eingabedaten zwischen Trainings- und Testevents, größere Auswirkungen hat als klassisches Overfitting; tiefere Modelle können unter diesen Bedingungen sogar besser generalisieren. Event-basierte Validierung ist daher essenziell, da sie die tatsächliche Generalisierungsfähigkeit offenbart und Random-Validierung zu optimistischen Einschätzungen führt. Insgesamt bleibt die fehlende Erfassung von Kontextfaktoren wie Fahrercharakteristiken, Setup-Parametern, Reifenstatus und Umgebungsbedingungen die zentrale Limitation, die mit reiner Telemetrie nicht überwunden werden kann.

Was dennoch bleibt sind die Erkenntnisse aus dem Entwicklungsprozess, die wertvolle Einblicke in die Herausforderungen und Limitationen bei der Anwendung von \ac{ML} im Motorsport-Kontext bieten und einen Grundstein für zukünftige Arbeiten legen.
